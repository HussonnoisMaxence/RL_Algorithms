{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TBRLAlgol.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ymoJi0FOISCx"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJZkgYWptm3qqNAzPbyPYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HussonnoisMaxence/RL_Algorithms/blob/master/TBRLAlgol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IVeqkc66m4I",
        "colab_type": "text"
      },
      "source": [
        "#Import & Constante\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmnmmmBH9JdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "l = [1,2,3]\n",
        "np.random.choice(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axzxf-Zp9rED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import normalize\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "import math\n",
        "HIDDEN_SIZE = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOT1PrHsjQD0",
        "colab_type": "text"
      },
      "source": [
        "# Value-Based Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWdi7ZBFPuii",
        "colab_type": "text"
      },
      "source": [
        "##Reinforcement learning theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IfV8d2YI3ZK",
        "colab_type": "text"
      },
      "source": [
        "The world is modelised by Markov Decision Process.\n",
        "\n",
        "MDP is a 5-tuple $ (S,A,{P_{SA}},\\gamma, R). $\n",
        "* S: is the set of states\n",
        "* A: is the set of actions\n",
        "* $P_{SA} $: is the state distribution \n",
        "* $\\gamma $: is the discount factor $\\gamma \\in [0,1] $\n",
        "* R: is the reward\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErE9FcmzMyY2",
        "colab_type": "text"
      },
      "source": [
        "Important notion\n",
        "\n",
        "* **Gain**: Sum of all reward\n",
        "> $G = \\sum_{i=0} \\gamma^iR$\n",
        "* **Policy** : A function that map state into action \n",
        "> $\\pi :S \\rightarrow A$\n",
        "* **State-Value function**: is the expected gain given a state S and a policy $\\pi$, \n",
        ">$V^\\pi(s) = E[G|\\pi, s_0=s ] $\n",
        "* **State-Action Value function** : is the expected gain given a state, an action, and a policy $\\pi$,\n",
        "> $Q^\\pi(s,a) = E[G|,\\pi, s_0=s, a_0=a]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Ml74BZMwP7",
        "colab_type": "text"
      },
      "source": [
        "Bellman Equation.\n",
        ">$V^\\pi(s) = E[G|\\pi, s_0=s ] $\n",
        "\n",
        ">$V^\\pi(s) = r + \\gamma \\sum_{s'\\in S}*V^\\pi(s')*P_{s',\\pi(s')} $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymoJi0FOISCx",
        "colab_type": "text"
      },
      "source": [
        "##Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4npDJAHTjL7N",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZZL4NKSECz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbS9ozA_FR4M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b375996c-3f4c-4b23-b17c-3ee3004f1f7c"
      },
      "source": [
        "class Q_table:\n",
        "  def __init__(self, action_space, alpha, gamma):\n",
        "    self.table = dict()\n",
        "    self.action_space_n = action_space\n",
        "    self.alpha = alpha\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def add_state(self, state):\n",
        "      if state not in self.table.keys():\n",
        "          self.table[state] = list(np.zeros(self.action_space_n))\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    return int(np.argmax(self.table[state]))\n",
        "\n",
        "  def update_value(self, next_state, state, action, reward):\n",
        "      old_q_value = self.table[state][action]\n",
        "      \n",
        "      if next_state not in self.table.keys():\n",
        "          self.table[next_state] = list(np.zeros(self.action_space_n))\n",
        "\n",
        "      next_max = np.max(self.table[next_state])    \n",
        "      new_value = (1 - self.alpha) * old_q_value + self.alpha* (reward + self.gamma* next_max) # Update Q-value\n",
        "      self.table[state][action] = new_value\n",
        "\n",
        "\n",
        "class QAgent:\n",
        "  def __init__(self, action_space,learning_rate, discount_factor, \n",
        "               epsilon, decay):\n",
        "    self.action_space = action_space\n",
        "    self.epsilon = epsilon\n",
        "    self.decay = decay\n",
        "    self.q_table = Q_table(action_space, learning_rate, discount_factor)\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "    # Greedy-policy\n",
        "    if(np.random.uniform() < self.epsilon):\n",
        "      action = np.random.choice(self.action_space)\n",
        "    else:\n",
        "      action = self.q_table.choose_action(obs)\n",
        "    return action\n",
        "  def update_epsilon(self):\n",
        "    self.epsilon = self.epsilon - self.decay*self.epsilon\n",
        "    self.epsilon =  max(self.epsilon, 0.01)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  def play_a_step(self, obs):      \\n    # Greedy-policy\\n    if(np.random.uniform() < self.epsilon):\\n      action = np.random.choice(self.action_space.n)\\n    else:\\n      action = self.q_table.choose_action(obs)\\n\\n    next_obs, reward, done,_ = self.env.step(action)\\n    return obs, reward, done, next_obs, action\\n\\n  def train(self, limit_step, batch):\\n      step = 0\\n      while True:\\n        for i in range(batch):\\n          step += 1\\n          reward_episode = 0\\n          obs = env.reset()\\n          done = False\\n          while not(done):\\n            self.q_table.add_state(obs)\\n            obs, reward, done, next_obs, action = self.play_a_step(obs)\\n            self.q_table.update_value(next_obs, obs, action, reward)\\n            obs = next_obs    \\n    \\n\\n            reward_episode += reward\\n        \\n        self.epsilon = self.epsilon - self.decay*self.epsilon\\n        self.epsilon =  max(self.epsilon, 0.01)\\n\\n        print(\"Step:\",step,\"Reward:\",reward_episode,\"size:\", self.epsilon)\\n        if step > limit_step:\\n          break;\\n            \\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDfHrZB0Y4Ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8135e2ea-ed5d-45dc-8468-8e21d751a084"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "\n",
        "agent = QAgent(action_space=env.action_space.n,\n",
        "               discount_factor=0.99, \n",
        "               epsilon=0.99,\n",
        "               decay=0.01, \n",
        "               learning_rate=0.001)\n",
        "\n",
        "epochs = 1000\n",
        "batch =  100\n",
        "\n",
        "\n",
        "for step in range(epochs):\n",
        "  score_batch = []\n",
        "  for i in range(batch):\n",
        "    reward_episode = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not(done):\n",
        "      agent.q_table.add_state(obs)\n",
        "\n",
        "      action = agent.choose_action(obs)\n",
        "\n",
        "      next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "      agent.q_table.update_value(next_obs, obs, action, reward)\n",
        "\n",
        "      obs = next_obs    \n",
        "      \n",
        "\n",
        "      reward_episode += reward\n",
        "    score_batch.append(reward_episode) \n",
        "  agent.update_epsilon()\n",
        "\n",
        "  print(\"Step:\",step,\"Average score:\", sum(score_batch)/len(score_batch),\"size:\", agent.epsilon)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0 Average score: 0.01 size: 0.9801\n",
            "Step: 1 Average score: 0.03 size: 0.970299\n",
            "Step: 2 Average score: 0.01 size: 0.9605960100000001\n",
            "Step: 3 Average score: 0.0 size: 0.9509900499\n",
            "Step: 4 Average score: 0.01 size: 0.941480149401\n",
            "Step: 5 Average score: 0.03 size: 0.93206534790699\n",
            "Step: 6 Average score: 0.01 size: 0.9227446944279202\n",
            "Step: 7 Average score: 0.02 size: 0.9135172474836409\n",
            "Step: 8 Average score: 0.05 size: 0.9043820750088045\n",
            "Step: 9 Average score: 0.02 size: 0.8953382542587165\n",
            "Step: 10 Average score: 0.02 size: 0.8863848717161293\n",
            "Step: 11 Average score: 0.05 size: 0.8775210229989681\n",
            "Step: 12 Average score: 0.09 size: 0.8687458127689783\n",
            "Step: 13 Average score: 0.03 size: 0.8600583546412885\n",
            "Step: 14 Average score: 0.1 size: 0.8514577710948756\n",
            "Step: 15 Average score: 0.03 size: 0.8429431933839269\n",
            "Step: 16 Average score: 0.11 size: 0.8345137614500876\n",
            "Step: 17 Average score: 0.07 size: 0.8261686238355868\n",
            "Step: 18 Average score: 0.07 size: 0.8179069375972309\n",
            "Step: 19 Average score: 0.09 size: 0.8097278682212585\n",
            "Step: 20 Average score: 0.09 size: 0.801630589539046\n",
            "Step: 21 Average score: 0.07 size: 0.7936142836436555\n",
            "Step: 22 Average score: 0.11 size: 0.785678140807219\n",
            "Step: 23 Average score: 0.12 size: 0.7778213593991468\n",
            "Step: 24 Average score: 0.09 size: 0.7700431458051553\n",
            "Step: 25 Average score: 0.12 size: 0.7623427143471038\n",
            "Step: 26 Average score: 0.11 size: 0.7547192872036327\n",
            "Step: 27 Average score: 0.13 size: 0.7471720943315964\n",
            "Step: 28 Average score: 0.17 size: 0.7397003733882804\n",
            "Step: 29 Average score: 0.18 size: 0.7323033696543976\n",
            "Step: 30 Average score: 0.1 size: 0.7249803359578536\n",
            "Step: 31 Average score: 0.18 size: 0.717730532598275\n",
            "Step: 32 Average score: 0.15 size: 0.7105532272722923\n",
            "Step: 33 Average score: 0.15 size: 0.7034476949995694\n",
            "Step: 34 Average score: 0.11 size: 0.6964132180495737\n",
            "Step: 35 Average score: 0.21 size: 0.6894490858690779\n",
            "Step: 36 Average score: 0.14 size: 0.6825545950103872\n",
            "Step: 37 Average score: 0.25 size: 0.6757290490602833\n",
            "Step: 38 Average score: 0.26 size: 0.6689717585696805\n",
            "Step: 39 Average score: 0.28 size: 0.6622820409839837\n",
            "Step: 40 Average score: 0.25 size: 0.6556592205741438\n",
            "Step: 41 Average score: 0.25 size: 0.6491026283684024\n",
            "Step: 42 Average score: 0.21 size: 0.6426116020847183\n",
            "Step: 43 Average score: 0.16 size: 0.6361854860638712\n",
            "Step: 44 Average score: 0.28 size: 0.6298236312032325\n",
            "Step: 45 Average score: 0.3 size: 0.6235253948912002\n",
            "Step: 46 Average score: 0.29 size: 0.6172901409422882\n",
            "Step: 47 Average score: 0.23 size: 0.6111172395328653\n",
            "Step: 48 Average score: 0.28 size: 0.6050060671375367\n",
            "Step: 49 Average score: 0.34 size: 0.5989560064661613\n",
            "Step: 50 Average score: 0.28 size: 0.5929664464014998\n",
            "Step: 51 Average score: 0.25 size: 0.5870367819374848\n",
            "Step: 52 Average score: 0.25 size: 0.5811664141181099\n",
            "Step: 53 Average score: 0.34 size: 0.5753547499769288\n",
            "Step: 54 Average score: 0.34 size: 0.5696012024771595\n",
            "Step: 55 Average score: 0.27 size: 0.563905190452388\n",
            "Step: 56 Average score: 0.38 size: 0.5582661385478641\n",
            "Step: 57 Average score: 0.31 size: 0.5526834771623854\n",
            "Step: 58 Average score: 0.33 size: 0.5471566423907616\n",
            "Step: 59 Average score: 0.31 size: 0.5416850759668539\n",
            "Step: 60 Average score: 0.31 size: 0.5362682252071853\n",
            "Step: 61 Average score: 0.4 size: 0.5309055429551135\n",
            "Step: 62 Average score: 0.39 size: 0.5255964875255623\n",
            "Step: 63 Average score: 0.33 size: 0.5203405226503067\n",
            "Step: 64 Average score: 0.38 size: 0.5151371174238036\n",
            "Step: 65 Average score: 0.31 size: 0.5099857462495656\n",
            "Step: 66 Average score: 0.41 size: 0.50488588878707\n",
            "Step: 67 Average score: 0.38 size: 0.49983702989919926\n",
            "Step: 68 Average score: 0.37 size: 0.4948386596002073\n",
            "Step: 69 Average score: 0.36 size: 0.48989027300420523\n",
            "Step: 70 Average score: 0.45 size: 0.48499137027416317\n",
            "Step: 71 Average score: 0.48 size: 0.48014145657142154\n",
            "Step: 72 Average score: 0.51 size: 0.47534004200570734\n",
            "Step: 73 Average score: 0.45 size: 0.4705866415856503\n",
            "Step: 74 Average score: 0.42 size: 0.46588077516979376\n",
            "Step: 75 Average score: 0.46 size: 0.46122196741809585\n",
            "Step: 76 Average score: 0.52 size: 0.4566097477439149\n",
            "Step: 77 Average score: 0.5 size: 0.45204365026647575\n",
            "Step: 78 Average score: 0.51 size: 0.447523213763811\n",
            "Step: 79 Average score: 0.53 size: 0.44304798162617287\n",
            "Step: 80 Average score: 0.43 size: 0.43861750180991116\n",
            "Step: 81 Average score: 0.43 size: 0.43423132679181203\n",
            "Step: 82 Average score: 0.46 size: 0.4298890135238939\n",
            "Step: 83 Average score: 0.6 size: 0.425590123388655\n",
            "Step: 84 Average score: 0.53 size: 0.42133422215476846\n",
            "Step: 85 Average score: 0.49 size: 0.4171208799332208\n",
            "Step: 86 Average score: 0.5 size: 0.4129496711338886\n",
            "Step: 87 Average score: 0.54 size: 0.4088201744225497\n",
            "Step: 88 Average score: 0.59 size: 0.4047319726783242\n",
            "Step: 89 Average score: 0.56 size: 0.400684652951541\n",
            "Step: 90 Average score: 0.57 size: 0.3966778064220256\n",
            "Step: 91 Average score: 0.56 size: 0.39271102835780536\n",
            "Step: 92 Average score: 0.54 size: 0.3887839180742273\n",
            "Step: 93 Average score: 0.53 size: 0.38489607889348504\n",
            "Step: 94 Average score: 0.55 size: 0.38104711810455016\n",
            "Step: 95 Average score: 0.56 size: 0.37723664692350467\n",
            "Step: 96 Average score: 0.61 size: 0.3734642804542696\n",
            "Step: 97 Average score: 0.63 size: 0.3697296376497269\n",
            "Step: 98 Average score: 0.63 size: 0.3660323412732296\n",
            "Step: 99 Average score: 0.57 size: 0.3623720178604973\n",
            "Step: 100 Average score: 0.52 size: 0.3587482976818923\n",
            "Step: 101 Average score: 0.66 size: 0.3551608147050734\n",
            "Step: 102 Average score: 0.58 size: 0.35160920655802264\n",
            "Step: 103 Average score: 0.57 size: 0.3480931144924424\n",
            "Step: 104 Average score: 0.56 size: 0.344612183347518\n",
            "Step: 105 Average score: 0.6 size: 0.3411660615140428\n",
            "Step: 106 Average score: 0.56 size: 0.33775440089890235\n",
            "Step: 107 Average score: 0.71 size: 0.3343768568899133\n",
            "Step: 108 Average score: 0.63 size: 0.3310330883210142\n",
            "Step: 109 Average score: 0.65 size: 0.32772275743780405\n",
            "Step: 110 Average score: 0.72 size: 0.324445529863426\n",
            "Step: 111 Average score: 0.72 size: 0.32120107456479174\n",
            "Step: 112 Average score: 0.61 size: 0.3179890638191438\n",
            "Step: 113 Average score: 0.62 size: 0.31480917318095236\n",
            "Step: 114 Average score: 0.65 size: 0.31166108144914284\n",
            "Step: 115 Average score: 0.62 size: 0.3085444706346514\n",
            "Step: 116 Average score: 0.6 size: 0.3054590259283049\n",
            "Step: 117 Average score: 0.65 size: 0.3024044356690218\n",
            "Step: 118 Average score: 0.71 size: 0.2993803913123316\n",
            "Step: 119 Average score: 0.65 size: 0.29638658739920826\n",
            "Step: 120 Average score: 0.6 size: 0.2934227215252162\n",
            "Step: 121 Average score: 0.66 size: 0.29048849430996404\n",
            "Step: 122 Average score: 0.77 size: 0.2875836093668644\n",
            "Step: 123 Average score: 0.72 size: 0.28470777327319574\n",
            "Step: 124 Average score: 0.69 size: 0.2818606955404638\n",
            "Step: 125 Average score: 0.65 size: 0.27904208858505913\n",
            "Step: 126 Average score: 0.71 size: 0.27625166769920856\n",
            "Step: 127 Average score: 0.67 size: 0.2734891510222165\n",
            "Step: 128 Average score: 0.73 size: 0.27075425951199433\n",
            "Step: 129 Average score: 0.7 size: 0.2680467169168744\n",
            "Step: 130 Average score: 0.71 size: 0.2653662497477056\n",
            "Step: 131 Average score: 0.66 size: 0.26271258725022856\n",
            "Step: 132 Average score: 0.72 size: 0.26008546137772626\n",
            "Step: 133 Average score: 0.75 size: 0.257484606763949\n",
            "Step: 134 Average score: 0.83 size: 0.25490976069630955\n",
            "Step: 135 Average score: 0.76 size: 0.25236066308934646\n",
            "Step: 136 Average score: 0.72 size: 0.249837056458453\n",
            "Step: 137 Average score: 0.7 size: 0.24733868589386848\n",
            "Step: 138 Average score: 0.79 size: 0.2448652990349298\n",
            "Step: 139 Average score: 0.61 size: 0.2424166460445805\n",
            "Step: 140 Average score: 0.76 size: 0.2399924795841347\n",
            "Step: 141 Average score: 0.8 size: 0.23759255478829336\n",
            "Step: 142 Average score: 0.73 size: 0.23521662924041042\n",
            "Step: 143 Average score: 0.76 size: 0.2328644629480063\n",
            "Step: 144 Average score: 0.75 size: 0.23053581831852624\n",
            "Step: 145 Average score: 0.69 size: 0.22823046013534098\n",
            "Step: 146 Average score: 0.8 size: 0.2259481555339876\n",
            "Step: 147 Average score: 0.74 size: 0.22368867397864772\n",
            "Step: 148 Average score: 0.74 size: 0.22145178723886125\n",
            "Step: 149 Average score: 0.78 size: 0.21923726936647264\n",
            "Step: 150 Average score: 0.71 size: 0.2170448966728079\n",
            "Step: 151 Average score: 0.74 size: 0.21487444770607983\n",
            "Step: 152 Average score: 0.81 size: 0.21272570322901904\n",
            "Step: 153 Average score: 0.77 size: 0.21059844619672885\n",
            "Step: 154 Average score: 0.72 size: 0.20849246173476157\n",
            "Step: 155 Average score: 0.78 size: 0.20640753711741397\n",
            "Step: 156 Average score: 0.78 size: 0.20434346174623982\n",
            "Step: 157 Average score: 0.77 size: 0.20230002712877743\n",
            "Step: 158 Average score: 0.77 size: 0.20027702685748966\n",
            "Step: 159 Average score: 0.75 size: 0.19827425658891476\n",
            "Step: 160 Average score: 0.83 size: 0.1962915140230256\n",
            "Step: 161 Average score: 0.8 size: 0.19432859888279536\n",
            "Step: 162 Average score: 0.8 size: 0.1923853128939674\n",
            "Step: 163 Average score: 0.81 size: 0.19046145976502774\n",
            "Step: 164 Average score: 0.86 size: 0.18855684516737745\n",
            "Step: 165 Average score: 0.8 size: 0.18667127671570366\n",
            "Step: 166 Average score: 0.73 size: 0.18480456394854664\n",
            "Step: 167 Average score: 0.81 size: 0.18295651830906118\n",
            "Step: 168 Average score: 0.81 size: 0.18112695312597057\n",
            "Step: 169 Average score: 0.8 size: 0.17931568359471087\n",
            "Step: 170 Average score: 0.73 size: 0.17752252675876376\n",
            "Step: 171 Average score: 0.84 size: 0.17574730149117612\n",
            "Step: 172 Average score: 0.77 size: 0.17398982847626437\n",
            "Step: 173 Average score: 0.87 size: 0.17224993019150173\n",
            "Step: 174 Average score: 0.78 size: 0.17052743088958672\n",
            "Step: 175 Average score: 0.81 size: 0.16882215658069086\n",
            "Step: 176 Average score: 0.83 size: 0.16713393501488394\n",
            "Step: 177 Average score: 0.81 size: 0.1654625956647351\n",
            "Step: 178 Average score: 0.86 size: 0.16380796970808775\n",
            "Step: 179 Average score: 0.83 size: 0.16216989001100687\n",
            "Step: 180 Average score: 0.81 size: 0.1605481911108968\n",
            "Step: 181 Average score: 0.82 size: 0.15894270919978784\n",
            "Step: 182 Average score: 0.8 size: 0.15735328210778995\n",
            "Step: 183 Average score: 0.77 size: 0.15577974928671207\n",
            "Step: 184 Average score: 0.83 size: 0.15422195179384496\n",
            "Step: 185 Average score: 0.84 size: 0.1526797322759065\n",
            "Step: 186 Average score: 0.82 size: 0.15115293495314744\n",
            "Step: 187 Average score: 0.81 size: 0.14964140560361597\n",
            "Step: 188 Average score: 0.82 size: 0.14814499154757982\n",
            "Step: 189 Average score: 0.9 size: 0.14666354163210402\n",
            "Step: 190 Average score: 0.9 size: 0.145196906215783\n",
            "Step: 191 Average score: 0.88 size: 0.14374493715362516\n",
            "Step: 192 Average score: 0.79 size: 0.1423074877820889\n",
            "Step: 193 Average score: 0.81 size: 0.140884412904268\n",
            "Step: 194 Average score: 0.83 size: 0.13947556877522532\n",
            "Step: 195 Average score: 0.83 size: 0.13808081308747308\n",
            "Step: 196 Average score: 0.87 size: 0.13670000495659834\n",
            "Step: 197 Average score: 0.84 size: 0.13533300490703237\n",
            "Step: 198 Average score: 0.87 size: 0.13397967485796206\n",
            "Step: 199 Average score: 0.87 size: 0.13263987810938244\n",
            "Step: 200 Average score: 0.89 size: 0.1313134793282886\n",
            "Step: 201 Average score: 0.8 size: 0.13000034453500572\n",
            "Step: 202 Average score: 0.84 size: 0.12870034108965567\n",
            "Step: 203 Average score: 0.92 size: 0.12741333767875912\n",
            "Step: 204 Average score: 0.84 size: 0.12613920430197154\n",
            "Step: 205 Average score: 0.83 size: 0.12487781225895182\n",
            "Step: 206 Average score: 0.83 size: 0.12362903413636231\n",
            "Step: 207 Average score: 0.91 size: 0.12239274379499869\n",
            "Step: 208 Average score: 0.83 size: 0.12116881635704871\n",
            "Step: 209 Average score: 0.92 size: 0.11995712819347822\n",
            "Step: 210 Average score: 0.86 size: 0.11875755691154344\n",
            "Step: 211 Average score: 0.85 size: 0.117569981342428\n",
            "Step: 212 Average score: 0.92 size: 0.11639428152900373\n",
            "Step: 213 Average score: 0.87 size: 0.1152303387137137\n",
            "Step: 214 Average score: 0.83 size: 0.11407803532657657\n",
            "Step: 215 Average score: 0.88 size: 0.1129372549733108\n",
            "Step: 216 Average score: 0.9 size: 0.11180788242357768\n",
            "Step: 217 Average score: 0.9 size: 0.11068980359934191\n",
            "Step: 218 Average score: 0.86 size: 0.10958290556334849\n",
            "Step: 219 Average score: 0.92 size: 0.108487076507715\n",
            "Step: 220 Average score: 0.92 size: 0.10740220574263784\n",
            "Step: 221 Average score: 0.89 size: 0.10632818368521146\n",
            "Step: 222 Average score: 0.94 size: 0.10526490184835935\n",
            "Step: 223 Average score: 0.89 size: 0.10421225282987576\n",
            "Step: 224 Average score: 0.93 size: 0.103170130301577\n",
            "Step: 225 Average score: 0.87 size: 0.10213842899856122\n",
            "Step: 226 Average score: 0.89 size: 0.1011170447085756\n",
            "Step: 227 Average score: 0.89 size: 0.10010587426148984\n",
            "Step: 228 Average score: 0.84 size: 0.09910481551887494\n",
            "Step: 229 Average score: 0.92 size: 0.0981137673636862\n",
            "Step: 230 Average score: 0.9 size: 0.09713262969004934\n",
            "Step: 231 Average score: 0.92 size: 0.09616130339314884\n",
            "Step: 232 Average score: 0.85 size: 0.09519969035921735\n",
            "Step: 233 Average score: 0.92 size: 0.09424769345562518\n",
            "Step: 234 Average score: 0.88 size: 0.09330521652106893\n",
            "Step: 235 Average score: 0.9 size: 0.09237216435585824\n",
            "Step: 236 Average score: 0.87 size: 0.09144844271229965\n",
            "Step: 237 Average score: 0.87 size: 0.09053395828517666\n",
            "Step: 238 Average score: 0.87 size: 0.08962861870232489\n",
            "Step: 239 Average score: 0.92 size: 0.08873233251530163\n",
            "Step: 240 Average score: 0.91 size: 0.08784500919014862\n",
            "Step: 241 Average score: 0.92 size: 0.08696655909824713\n",
            "Step: 242 Average score: 0.93 size: 0.08609689350726465\n",
            "Step: 243 Average score: 0.9 size: 0.085235924572192\n",
            "Step: 244 Average score: 0.92 size: 0.08438356532647008\n",
            "Step: 245 Average score: 0.95 size: 0.08353972967320537\n",
            "Step: 246 Average score: 0.91 size: 0.08270433237647332\n",
            "Step: 247 Average score: 0.9 size: 0.08187728905270858\n",
            "Step: 248 Average score: 0.96 size: 0.08105851616218149\n",
            "Step: 249 Average score: 0.94 size: 0.08024793100055967\n",
            "Step: 250 Average score: 0.92 size: 0.07944545169055407\n",
            "Step: 251 Average score: 0.92 size: 0.07865099717364853\n",
            "Step: 252 Average score: 0.93 size: 0.07786448720191204\n",
            "Step: 253 Average score: 0.87 size: 0.07708584232989292\n",
            "Step: 254 Average score: 0.92 size: 0.076314983906594\n",
            "Step: 255 Average score: 0.93 size: 0.07555183406752805\n",
            "Step: 256 Average score: 0.94 size: 0.07479631572685277\n",
            "Step: 257 Average score: 0.9 size: 0.07404835256958425\n",
            "Step: 258 Average score: 0.92 size: 0.0733078690438884\n",
            "Step: 259 Average score: 0.9 size: 0.07257479035344952\n",
            "Step: 260 Average score: 0.9 size: 0.07184904244991502\n",
            "Step: 261 Average score: 0.87 size: 0.07113055202541588\n",
            "Step: 262 Average score: 0.94 size: 0.07041924650516172\n",
            "Step: 263 Average score: 0.89 size: 0.06971505404011011\n",
            "Step: 264 Average score: 0.96 size: 0.069017903499709\n",
            "Step: 265 Average score: 0.93 size: 0.06832772446471191\n",
            "Step: 266 Average score: 0.94 size: 0.0676444472200648\n",
            "Step: 267 Average score: 0.91 size: 0.06696800274786414\n",
            "Step: 268 Average score: 0.92 size: 0.06629832272038551\n",
            "Step: 269 Average score: 0.96 size: 0.06563533949318165\n",
            "Step: 270 Average score: 0.91 size: 0.06497898609824983\n",
            "Step: 271 Average score: 0.9 size: 0.06432919623726734\n",
            "Step: 272 Average score: 0.92 size: 0.06368590427489466\n",
            "Step: 273 Average score: 0.9 size: 0.06304904523214572\n",
            "Step: 274 Average score: 0.93 size: 0.06241855477982426\n",
            "Step: 275 Average score: 0.97 size: 0.06179436923202602\n",
            "Step: 276 Average score: 0.98 size: 0.06117642553970576\n",
            "Step: 277 Average score: 0.93 size: 0.0605646612843087\n",
            "Step: 278 Average score: 0.92 size: 0.05995901467146561\n",
            "Step: 279 Average score: 0.95 size: 0.05935942452475095\n",
            "Step: 280 Average score: 0.95 size: 0.05876583027950344\n",
            "Step: 281 Average score: 0.95 size: 0.0581781719767084\n",
            "Step: 282 Average score: 0.97 size: 0.05759639025694132\n",
            "Step: 283 Average score: 0.95 size: 0.057020426354371906\n",
            "Step: 284 Average score: 0.96 size: 0.05645022209082819\n",
            "Step: 285 Average score: 0.96 size: 0.05588571986991991\n",
            "Step: 286 Average score: 0.97 size: 0.05532686267122071\n",
            "Step: 287 Average score: 0.96 size: 0.0547735940445085\n",
            "Step: 288 Average score: 0.96 size: 0.05422585810406342\n",
            "Step: 289 Average score: 0.98 size: 0.05368359952302278\n",
            "Step: 290 Average score: 0.93 size: 0.05314676352779255\n",
            "Step: 291 Average score: 0.88 size: 0.052615295892514624\n",
            "Step: 292 Average score: 0.95 size: 0.052089142933589475\n",
            "Step: 293 Average score: 0.94 size: 0.05156825150425358\n",
            "Step: 294 Average score: 0.96 size: 0.051052568989211046\n",
            "Step: 295 Average score: 0.96 size: 0.05054204329931893\n",
            "Step: 296 Average score: 0.94 size: 0.05003662286632574\n",
            "Step: 297 Average score: 0.96 size: 0.049536256637662486\n",
            "Step: 298 Average score: 0.97 size: 0.04904089407128586\n",
            "Step: 299 Average score: 0.98 size: 0.048550485130573\n",
            "Step: 300 Average score: 0.97 size: 0.04806498027926727\n",
            "Step: 301 Average score: 0.94 size: 0.0475843304764746\n",
            "Step: 302 Average score: 0.95 size: 0.04710848717170985\n",
            "Step: 303 Average score: 0.97 size: 0.046637402299992754\n",
            "Step: 304 Average score: 0.91 size: 0.04617102827699283\n",
            "Step: 305 Average score: 0.98 size: 0.0457093179942229\n",
            "Step: 306 Average score: 0.96 size: 0.04525222481428067\n",
            "Step: 307 Average score: 0.94 size: 0.04479970256613786\n",
            "Step: 308 Average score: 0.96 size: 0.04435170554047648\n",
            "Step: 309 Average score: 0.99 size: 0.04390818848507172\n",
            "Step: 310 Average score: 0.96 size: 0.043469106600221005\n",
            "Step: 311 Average score: 0.93 size: 0.043034415534218794\n",
            "Step: 312 Average score: 0.96 size: 0.042604071378876604\n",
            "Step: 313 Average score: 0.96 size: 0.04217803066508784\n",
            "Step: 314 Average score: 0.96 size: 0.041756250358436955\n",
            "Step: 315 Average score: 0.94 size: 0.041338687854852584\n",
            "Step: 316 Average score: 0.95 size: 0.040925300976304056\n",
            "Step: 317 Average score: 0.94 size: 0.04051604796654101\n",
            "Step: 318 Average score: 0.95 size: 0.0401108874868756\n",
            "Step: 319 Average score: 0.96 size: 0.03970977861200684\n",
            "Step: 320 Average score: 0.97 size: 0.03931268082588677\n",
            "Step: 321 Average score: 0.94 size: 0.0389195540176279\n",
            "Step: 322 Average score: 0.98 size: 0.03853035847745162\n",
            "Step: 323 Average score: 0.97 size: 0.038145054892677104\n",
            "Step: 324 Average score: 0.99 size: 0.03776360434375033\n",
            "Step: 325 Average score: 0.94 size: 0.03738596830031283\n",
            "Step: 326 Average score: 0.98 size: 0.0370121086173097\n",
            "Step: 327 Average score: 0.99 size: 0.0366419875311366\n",
            "Step: 328 Average score: 0.96 size: 0.036275567655825236\n",
            "Step: 329 Average score: 0.98 size: 0.03591281197926698\n",
            "Step: 330 Average score: 0.97 size: 0.035553683859474314\n",
            "Step: 331 Average score: 0.97 size: 0.03519814702087957\n",
            "Step: 332 Average score: 0.98 size: 0.03484616555067077\n",
            "Step: 333 Average score: 0.97 size: 0.034497703895164065\n",
            "Step: 334 Average score: 0.98 size: 0.03415272685621242\n",
            "Step: 335 Average score: 0.99 size: 0.0338111995876503\n",
            "Step: 336 Average score: 0.98 size: 0.033473087591773795\n",
            "Step: 337 Average score: 0.94 size: 0.033138356715856056\n",
            "Step: 338 Average score: 0.95 size: 0.032806973148697495\n",
            "Step: 339 Average score: 0.98 size: 0.03247890341721052\n",
            "Step: 340 Average score: 0.98 size: 0.03215411438303841\n",
            "Step: 341 Average score: 0.95 size: 0.031832573239208024\n",
            "Step: 342 Average score: 0.98 size: 0.031514247506815946\n",
            "Step: 343 Average score: 0.97 size: 0.031199105031747786\n",
            "Step: 344 Average score: 0.97 size: 0.03088711398143031\n",
            "Step: 345 Average score: 0.96 size: 0.030578242841616005\n",
            "Step: 346 Average score: 0.95 size: 0.030272460413199843\n",
            "Step: 347 Average score: 0.98 size: 0.029969735809067845\n",
            "Step: 348 Average score: 0.98 size: 0.029670038450977168\n",
            "Step: 349 Average score: 0.91 size: 0.029373338066467396\n",
            "Step: 350 Average score: 1.0 size: 0.029079604685802722\n",
            "Step: 351 Average score: 0.98 size: 0.028788808638944695\n",
            "Step: 352 Average score: 0.96 size: 0.028500920552555247\n",
            "Step: 353 Average score: 0.93 size: 0.028215911347029693\n",
            "Step: 354 Average score: 0.98 size: 0.027933752233559397\n",
            "Step: 355 Average score: 0.98 size: 0.027654414711223804\n",
            "Step: 356 Average score: 1.0 size: 0.027377870564111565\n",
            "Step: 357 Average score: 0.99 size: 0.027104091858470448\n",
            "Step: 358 Average score: 0.98 size: 0.026833050939885743\n",
            "Step: 359 Average score: 0.96 size: 0.026564720430486886\n",
            "Step: 360 Average score: 0.99 size: 0.026299073226182017\n",
            "Step: 361 Average score: 0.96 size: 0.026036082493920195\n",
            "Step: 362 Average score: 0.98 size: 0.025775721668980994\n",
            "Step: 363 Average score: 0.96 size: 0.025517964452291184\n",
            "Step: 364 Average score: 0.98 size: 0.025262784807768272\n",
            "Step: 365 Average score: 0.96 size: 0.02501015695969059\n",
            "Step: 366 Average score: 0.98 size: 0.024760055390093683\n",
            "Step: 367 Average score: 0.99 size: 0.024512454836192746\n",
            "Step: 368 Average score: 0.96 size: 0.02426733028783082\n",
            "Step: 369 Average score: 0.98 size: 0.02402465698495251\n",
            "Step: 370 Average score: 0.98 size: 0.023784410415102986\n",
            "Step: 371 Average score: 0.95 size: 0.023546566310951957\n",
            "Step: 372 Average score: 0.96 size: 0.023311100647842437\n",
            "Step: 373 Average score: 0.99 size: 0.023077989641364014\n",
            "Step: 374 Average score: 0.99 size: 0.022847209744950372\n",
            "Step: 375 Average score: 0.98 size: 0.02261873764750087\n",
            "Step: 376 Average score: 0.96 size: 0.02239255027102586\n",
            "Step: 377 Average score: 0.97 size: 0.0221686247683156\n",
            "Step: 378 Average score: 1.0 size: 0.021946938520632443\n",
            "Step: 379 Average score: 0.97 size: 0.021727469135426117\n",
            "Step: 380 Average score: 0.97 size: 0.021510194444071856\n",
            "Step: 381 Average score: 0.98 size: 0.021295092499631137\n",
            "Step: 382 Average score: 0.95 size: 0.021082141574634825\n",
            "Step: 383 Average score: 0.99 size: 0.020871320158888478\n",
            "Step: 384 Average score: 0.98 size: 0.020662606957299594\n",
            "Step: 385 Average score: 0.98 size: 0.0204559808877266\n",
            "Step: 386 Average score: 0.98 size: 0.020251421078849332\n",
            "Step: 387 Average score: 0.99 size: 0.02004890686806084\n",
            "Step: 388 Average score: 0.99 size: 0.019848417799380232\n",
            "Step: 389 Average score: 0.98 size: 0.01964993362138643\n",
            "Step: 390 Average score: 0.99 size: 0.019453434285172565\n",
            "Step: 391 Average score: 0.98 size: 0.01925889994232084\n",
            "Step: 392 Average score: 1.0 size: 0.01906631094289763\n",
            "Step: 393 Average score: 0.99 size: 0.018875647833468654\n",
            "Step: 394 Average score: 0.98 size: 0.018686891355133968\n",
            "Step: 395 Average score: 0.99 size: 0.01850002244158263\n",
            "Step: 396 Average score: 1.0 size: 0.018315022217166802\n",
            "Step: 397 Average score: 0.99 size: 0.018131871994995136\n",
            "Step: 398 Average score: 1.0 size: 0.017950553275045186\n",
            "Step: 399 Average score: 0.99 size: 0.017771047742294734\n",
            "Step: 400 Average score: 0.98 size: 0.017593337264871788\n",
            "Step: 401 Average score: 0.97 size: 0.01741740389222307\n",
            "Step: 402 Average score: 0.97 size: 0.017243229853300842\n",
            "Step: 403 Average score: 0.98 size: 0.017070797554767835\n",
            "Step: 404 Average score: 0.97 size: 0.016900089579220155\n",
            "Step: 405 Average score: 1.0 size: 0.016731088683427955\n",
            "Step: 406 Average score: 0.98 size: 0.016563777796593675\n",
            "Step: 407 Average score: 0.99 size: 0.016398140018627737\n",
            "Step: 408 Average score: 0.99 size: 0.01623415861844146\n",
            "Step: 409 Average score: 0.98 size: 0.016071817032257046\n",
            "Step: 410 Average score: 1.0 size: 0.015911098861934477\n",
            "Step: 411 Average score: 1.0 size: 0.015751987873315134\n",
            "Step: 412 Average score: 1.0 size: 0.015594467994581983\n",
            "Step: 413 Average score: 0.97 size: 0.015438523314636163\n",
            "Step: 414 Average score: 0.99 size: 0.015284138081489802\n",
            "Step: 415 Average score: 0.99 size: 0.015131296700674904\n",
            "Step: 416 Average score: 0.99 size: 0.014979983733668155\n",
            "Step: 417 Average score: 0.99 size: 0.014830183896331473\n",
            "Step: 418 Average score: 0.96 size: 0.014681882057368157\n",
            "Step: 419 Average score: 1.0 size: 0.014535063236794475\n",
            "Step: 420 Average score: 0.96 size: 0.01438971260442653\n",
            "Step: 421 Average score: 0.99 size: 0.014245815478382265\n",
            "Step: 422 Average score: 0.99 size: 0.014103357323598442\n",
            "Step: 423 Average score: 0.99 size: 0.013962323750362457\n",
            "Step: 424 Average score: 0.98 size: 0.013822700512858832\n",
            "Step: 425 Average score: 0.99 size: 0.013684473507730244\n",
            "Step: 426 Average score: 0.99 size: 0.013547628772652942\n",
            "Step: 427 Average score: 0.98 size: 0.013412152484926413\n",
            "Step: 428 Average score: 1.0 size: 0.01327803096007715\n",
            "Step: 429 Average score: 1.0 size: 0.013145250650476378\n",
            "Step: 430 Average score: 0.97 size: 0.013013798143971614\n",
            "Step: 431 Average score: 0.97 size: 0.012883660162531898\n",
            "Step: 432 Average score: 1.0 size: 0.012754823560906578\n",
            "Step: 433 Average score: 1.0 size: 0.012627275325297513\n",
            "Step: 434 Average score: 0.96 size: 0.012501002572044537\n",
            "Step: 435 Average score: 1.0 size: 0.012375992546324092\n",
            "Step: 436 Average score: 1.0 size: 0.012252232620860852\n",
            "Step: 437 Average score: 0.99 size: 0.012129710294652244\n",
            "Step: 438 Average score: 1.0 size: 0.012008413191705722\n",
            "Step: 439 Average score: 0.98 size: 0.011888329059788665\n",
            "Step: 440 Average score: 0.98 size: 0.011769445769190779\n",
            "Step: 441 Average score: 1.0 size: 0.01165175131149887\n",
            "Step: 442 Average score: 0.98 size: 0.011535233798383882\n",
            "Step: 443 Average score: 0.98 size: 0.011419881460400044\n",
            "Step: 444 Average score: 0.98 size: 0.011305682645796044\n",
            "Step: 445 Average score: 1.0 size: 0.011192625819338085\n",
            "Step: 446 Average score: 0.99 size: 0.011080699561144703\n",
            "Step: 447 Average score: 0.99 size: 0.010969892565533256\n",
            "Step: 448 Average score: 1.0 size: 0.010860193639877924\n",
            "Step: 449 Average score: 0.99 size: 0.010751591703479144\n",
            "Step: 450 Average score: 1.0 size: 0.010644075786444353\n",
            "Step: 451 Average score: 1.0 size: 0.01053763502857991\n",
            "Step: 452 Average score: 1.0 size: 0.01043225867829411\n",
            "Step: 453 Average score: 0.98 size: 0.01032793609151117\n",
            "Step: 454 Average score: 0.99 size: 0.010224656730596058\n",
            "Step: 455 Average score: 1.0 size: 0.010122410163290097\n",
            "Step: 456 Average score: 0.99 size: 0.010021186061657196\n",
            "Step: 457 Average score: 1.0 size: 0.01\n",
            "Step: 458 Average score: 1.0 size: 0.01\n",
            "Step: 459 Average score: 0.98 size: 0.01\n",
            "Step: 460 Average score: 0.98 size: 0.01\n",
            "Step: 461 Average score: 0.99 size: 0.01\n",
            "Step: 462 Average score: 0.98 size: 0.01\n",
            "Step: 463 Average score: 0.99 size: 0.01\n",
            "Step: 464 Average score: 0.98 size: 0.01\n",
            "Step: 465 Average score: 0.99 size: 0.01\n",
            "Step: 466 Average score: 0.99 size: 0.01\n",
            "Step: 467 Average score: 0.99 size: 0.01\n",
            "Step: 468 Average score: 1.0 size: 0.01\n",
            "Step: 469 Average score: 1.0 size: 0.01\n",
            "Step: 470 Average score: 1.0 size: 0.01\n",
            "Step: 471 Average score: 0.98 size: 0.01\n",
            "Step: 472 Average score: 1.0 size: 0.01\n",
            "Step: 473 Average score: 0.99 size: 0.01\n",
            "Step: 474 Average score: 0.99 size: 0.01\n",
            "Step: 475 Average score: 0.99 size: 0.01\n",
            "Step: 476 Average score: 0.99 size: 0.01\n",
            "Step: 477 Average score: 0.99 size: 0.01\n",
            "Step: 478 Average score: 1.0 size: 0.01\n",
            "Step: 479 Average score: 0.99 size: 0.01\n",
            "Step: 480 Average score: 0.99 size: 0.01\n",
            "Step: 481 Average score: 0.99 size: 0.01\n",
            "Step: 482 Average score: 0.97 size: 0.01\n",
            "Step: 483 Average score: 0.99 size: 0.01\n",
            "Step: 484 Average score: 1.0 size: 0.01\n",
            "Step: 485 Average score: 1.0 size: 0.01\n",
            "Step: 486 Average score: 1.0 size: 0.01\n",
            "Step: 487 Average score: 0.99 size: 0.01\n",
            "Step: 488 Average score: 1.0 size: 0.01\n",
            "Step: 489 Average score: 1.0 size: 0.01\n",
            "Step: 490 Average score: 0.99 size: 0.01\n",
            "Step: 491 Average score: 1.0 size: 0.01\n",
            "Step: 492 Average score: 0.99 size: 0.01\n",
            "Step: 493 Average score: 0.97 size: 0.01\n",
            "Step: 494 Average score: 1.0 size: 0.01\n",
            "Step: 495 Average score: 0.98 size: 0.01\n",
            "Step: 496 Average score: 0.98 size: 0.01\n",
            "Step: 497 Average score: 0.98 size: 0.01\n",
            "Step: 498 Average score: 0.98 size: 0.01\n",
            "Step: 499 Average score: 0.99 size: 0.01\n",
            "Step: 500 Average score: 0.99 size: 0.01\n",
            "Step: 501 Average score: 1.0 size: 0.01\n",
            "Step: 502 Average score: 0.97 size: 0.01\n",
            "Step: 503 Average score: 0.99 size: 0.01\n",
            "Step: 504 Average score: 0.99 size: 0.01\n",
            "Step: 505 Average score: 0.97 size: 0.01\n",
            "Step: 506 Average score: 1.0 size: 0.01\n",
            "Step: 507 Average score: 1.0 size: 0.01\n",
            "Step: 508 Average score: 1.0 size: 0.01\n",
            "Step: 509 Average score: 0.99 size: 0.01\n",
            "Step: 510 Average score: 0.99 size: 0.01\n",
            "Step: 511 Average score: 0.99 size: 0.01\n",
            "Step: 512 Average score: 0.98 size: 0.01\n",
            "Step: 513 Average score: 0.98 size: 0.01\n",
            "Step: 514 Average score: 0.99 size: 0.01\n",
            "Step: 515 Average score: 1.0 size: 0.01\n",
            "Step: 516 Average score: 1.0 size: 0.01\n",
            "Step: 517 Average score: 0.97 size: 0.01\n",
            "Step: 518 Average score: 0.96 size: 0.01\n",
            "Step: 519 Average score: 0.97 size: 0.01\n",
            "Step: 520 Average score: 0.98 size: 0.01\n",
            "Step: 521 Average score: 0.99 size: 0.01\n",
            "Step: 522 Average score: 0.99 size: 0.01\n",
            "Step: 523 Average score: 0.99 size: 0.01\n",
            "Step: 524 Average score: 0.99 size: 0.01\n",
            "Step: 525 Average score: 0.97 size: 0.01\n",
            "Step: 526 Average score: 0.99 size: 0.01\n",
            "Step: 527 Average score: 1.0 size: 0.01\n",
            "Step: 528 Average score: 0.99 size: 0.01\n",
            "Step: 529 Average score: 1.0 size: 0.01\n",
            "Step: 530 Average score: 0.94 size: 0.01\n",
            "Step: 531 Average score: 1.0 size: 0.01\n",
            "Step: 532 Average score: 0.98 size: 0.01\n",
            "Step: 533 Average score: 0.99 size: 0.01\n",
            "Step: 534 Average score: 1.0 size: 0.01\n",
            "Step: 535 Average score: 0.98 size: 0.01\n",
            "Step: 536 Average score: 0.99 size: 0.01\n",
            "Step: 537 Average score: 0.97 size: 0.01\n",
            "Step: 538 Average score: 1.0 size: 0.01\n",
            "Step: 539 Average score: 0.99 size: 0.01\n",
            "Step: 540 Average score: 0.97 size: 0.01\n",
            "Step: 541 Average score: 0.98 size: 0.01\n",
            "Step: 542 Average score: 1.0 size: 0.01\n",
            "Step: 543 Average score: 0.99 size: 0.01\n",
            "Step: 544 Average score: 0.99 size: 0.01\n",
            "Step: 545 Average score: 0.99 size: 0.01\n",
            "Step: 546 Average score: 0.98 size: 0.01\n",
            "Step: 547 Average score: 0.99 size: 0.01\n",
            "Step: 548 Average score: 0.99 size: 0.01\n",
            "Step: 549 Average score: 0.99 size: 0.01\n",
            "Step: 550 Average score: 0.99 size: 0.01\n",
            "Step: 551 Average score: 0.99 size: 0.01\n",
            "Step: 552 Average score: 0.97 size: 0.01\n",
            "Step: 553 Average score: 1.0 size: 0.01\n",
            "Step: 554 Average score: 1.0 size: 0.01\n",
            "Step: 555 Average score: 0.99 size: 0.01\n",
            "Step: 556 Average score: 1.0 size: 0.01\n",
            "Step: 557 Average score: 1.0 size: 0.01\n",
            "Step: 558 Average score: 0.99 size: 0.01\n",
            "Step: 559 Average score: 0.96 size: 0.01\n",
            "Step: 560 Average score: 1.0 size: 0.01\n",
            "Step: 561 Average score: 1.0 size: 0.01\n",
            "Step: 562 Average score: 1.0 size: 0.01\n",
            "Step: 563 Average score: 0.99 size: 0.01\n",
            "Step: 564 Average score: 1.0 size: 0.01\n",
            "Step: 565 Average score: 0.98 size: 0.01\n",
            "Step: 566 Average score: 0.98 size: 0.01\n",
            "Step: 567 Average score: 0.97 size: 0.01\n",
            "Step: 568 Average score: 1.0 size: 0.01\n",
            "Step: 569 Average score: 0.99 size: 0.01\n",
            "Step: 570 Average score: 0.99 size: 0.01\n",
            "Step: 571 Average score: 1.0 size: 0.01\n",
            "Step: 572 Average score: 1.0 size: 0.01\n",
            "Step: 573 Average score: 1.0 size: 0.01\n",
            "Step: 574 Average score: 1.0 size: 0.01\n",
            "Step: 575 Average score: 1.0 size: 0.01\n",
            "Step: 576 Average score: 0.99 size: 0.01\n",
            "Step: 577 Average score: 1.0 size: 0.01\n",
            "Step: 578 Average score: 1.0 size: 0.01\n",
            "Step: 579 Average score: 1.0 size: 0.01\n",
            "Step: 580 Average score: 0.99 size: 0.01\n",
            "Step: 581 Average score: 0.99 size: 0.01\n",
            "Step: 582 Average score: 0.98 size: 0.01\n",
            "Step: 583 Average score: 1.0 size: 0.01\n",
            "Step: 584 Average score: 1.0 size: 0.01\n",
            "Step: 585 Average score: 1.0 size: 0.01\n",
            "Step: 586 Average score: 0.98 size: 0.01\n",
            "Step: 587 Average score: 0.99 size: 0.01\n",
            "Step: 588 Average score: 1.0 size: 0.01\n",
            "Step: 589 Average score: 0.99 size: 0.01\n",
            "Step: 590 Average score: 1.0 size: 0.01\n",
            "Step: 591 Average score: 1.0 size: 0.01\n",
            "Step: 592 Average score: 0.99 size: 0.01\n",
            "Step: 593 Average score: 0.99 size: 0.01\n",
            "Step: 594 Average score: 0.98 size: 0.01\n",
            "Step: 595 Average score: 1.0 size: 0.01\n",
            "Step: 596 Average score: 1.0 size: 0.01\n",
            "Step: 597 Average score: 0.98 size: 0.01\n",
            "Step: 598 Average score: 0.99 size: 0.01\n",
            "Step: 599 Average score: 0.99 size: 0.01\n",
            "Step: 600 Average score: 1.0 size: 0.01\n",
            "Step: 601 Average score: 1.0 size: 0.01\n",
            "Step: 602 Average score: 1.0 size: 0.01\n",
            "Step: 603 Average score: 0.99 size: 0.01\n",
            "Step: 604 Average score: 0.99 size: 0.01\n",
            "Step: 605 Average score: 1.0 size: 0.01\n",
            "Step: 606 Average score: 1.0 size: 0.01\n",
            "Step: 607 Average score: 1.0 size: 0.01\n",
            "Step: 608 Average score: 0.98 size: 0.01\n",
            "Step: 609 Average score: 0.99 size: 0.01\n",
            "Step: 610 Average score: 0.99 size: 0.01\n",
            "Step: 611 Average score: 1.0 size: 0.01\n",
            "Step: 612 Average score: 0.99 size: 0.01\n",
            "Step: 613 Average score: 0.98 size: 0.01\n",
            "Step: 614 Average score: 0.99 size: 0.01\n",
            "Step: 615 Average score: 0.98 size: 0.01\n",
            "Step: 616 Average score: 1.0 size: 0.01\n",
            "Step: 617 Average score: 0.99 size: 0.01\n",
            "Step: 618 Average score: 0.99 size: 0.01\n",
            "Step: 619 Average score: 0.96 size: 0.01\n",
            "Step: 620 Average score: 0.99 size: 0.01\n",
            "Step: 621 Average score: 0.98 size: 0.01\n",
            "Step: 622 Average score: 0.97 size: 0.01\n",
            "Step: 623 Average score: 1.0 size: 0.01\n",
            "Step: 624 Average score: 0.98 size: 0.01\n",
            "Step: 625 Average score: 0.99 size: 0.01\n",
            "Step: 626 Average score: 0.99 size: 0.01\n",
            "Step: 627 Average score: 0.99 size: 0.01\n",
            "Step: 628 Average score: 0.98 size: 0.01\n",
            "Step: 629 Average score: 0.99 size: 0.01\n",
            "Step: 630 Average score: 1.0 size: 0.01\n",
            "Step: 631 Average score: 0.99 size: 0.01\n",
            "Step: 632 Average score: 0.98 size: 0.01\n",
            "Step: 633 Average score: 1.0 size: 0.01\n",
            "Step: 634 Average score: 0.99 size: 0.01\n",
            "Step: 635 Average score: 1.0 size: 0.01\n",
            "Step: 636 Average score: 0.98 size: 0.01\n",
            "Step: 637 Average score: 0.99 size: 0.01\n",
            "Step: 638 Average score: 0.98 size: 0.01\n",
            "Step: 639 Average score: 1.0 size: 0.01\n",
            "Step: 640 Average score: 0.96 size: 0.01\n",
            "Step: 641 Average score: 0.99 size: 0.01\n",
            "Step: 642 Average score: 1.0 size: 0.01\n",
            "Step: 643 Average score: 0.98 size: 0.01\n",
            "Step: 644 Average score: 0.97 size: 0.01\n",
            "Step: 645 Average score: 1.0 size: 0.01\n",
            "Step: 646 Average score: 0.99 size: 0.01\n",
            "Step: 647 Average score: 1.0 size: 0.01\n",
            "Step: 648 Average score: 0.98 size: 0.01\n",
            "Step: 649 Average score: 0.99 size: 0.01\n",
            "Step: 650 Average score: 0.95 size: 0.01\n",
            "Step: 651 Average score: 1.0 size: 0.01\n",
            "Step: 652 Average score: 0.99 size: 0.01\n",
            "Step: 653 Average score: 0.99 size: 0.01\n",
            "Step: 654 Average score: 1.0 size: 0.01\n",
            "Step: 655 Average score: 1.0 size: 0.01\n",
            "Step: 656 Average score: 1.0 size: 0.01\n",
            "Step: 657 Average score: 0.98 size: 0.01\n",
            "Step: 658 Average score: 0.98 size: 0.01\n",
            "Step: 659 Average score: 1.0 size: 0.01\n",
            "Step: 660 Average score: 1.0 size: 0.01\n",
            "Step: 661 Average score: 1.0 size: 0.01\n",
            "Step: 662 Average score: 0.99 size: 0.01\n",
            "Step: 663 Average score: 1.0 size: 0.01\n",
            "Step: 664 Average score: 0.99 size: 0.01\n",
            "Step: 665 Average score: 0.98 size: 0.01\n",
            "Step: 666 Average score: 1.0 size: 0.01\n",
            "Step: 667 Average score: 0.99 size: 0.01\n",
            "Step: 668 Average score: 0.99 size: 0.01\n",
            "Step: 669 Average score: 0.99 size: 0.01\n",
            "Step: 670 Average score: 0.99 size: 0.01\n",
            "Step: 671 Average score: 0.98 size: 0.01\n",
            "Step: 672 Average score: 0.96 size: 0.01\n",
            "Step: 673 Average score: 0.98 size: 0.01\n",
            "Step: 674 Average score: 1.0 size: 0.01\n",
            "Step: 675 Average score: 0.99 size: 0.01\n",
            "Step: 676 Average score: 0.96 size: 0.01\n",
            "Step: 677 Average score: 0.97 size: 0.01\n",
            "Step: 678 Average score: 0.98 size: 0.01\n",
            "Step: 679 Average score: 0.99 size: 0.01\n",
            "Step: 680 Average score: 0.98 size: 0.01\n",
            "Step: 681 Average score: 1.0 size: 0.01\n",
            "Step: 682 Average score: 0.97 size: 0.01\n",
            "Step: 683 Average score: 1.0 size: 0.01\n",
            "Step: 684 Average score: 0.98 size: 0.01\n",
            "Step: 685 Average score: 0.99 size: 0.01\n",
            "Step: 686 Average score: 0.98 size: 0.01\n",
            "Step: 687 Average score: 0.99 size: 0.01\n",
            "Step: 688 Average score: 0.99 size: 0.01\n",
            "Step: 689 Average score: 1.0 size: 0.01\n",
            "Step: 690 Average score: 0.99 size: 0.01\n",
            "Step: 691 Average score: 0.97 size: 0.01\n",
            "Step: 692 Average score: 0.99 size: 0.01\n",
            "Step: 693 Average score: 1.0 size: 0.01\n",
            "Step: 694 Average score: 1.0 size: 0.01\n",
            "Step: 695 Average score: 0.97 size: 0.01\n",
            "Step: 696 Average score: 0.97 size: 0.01\n",
            "Step: 697 Average score: 0.99 size: 0.01\n",
            "Step: 698 Average score: 0.99 size: 0.01\n",
            "Step: 699 Average score: 0.99 size: 0.01\n",
            "Step: 700 Average score: 1.0 size: 0.01\n",
            "Step: 701 Average score: 1.0 size: 0.01\n",
            "Step: 702 Average score: 0.99 size: 0.01\n",
            "Step: 703 Average score: 0.98 size: 0.01\n",
            "Step: 704 Average score: 1.0 size: 0.01\n",
            "Step: 705 Average score: 0.99 size: 0.01\n",
            "Step: 706 Average score: 1.0 size: 0.01\n",
            "Step: 707 Average score: 0.97 size: 0.01\n",
            "Step: 708 Average score: 1.0 size: 0.01\n",
            "Step: 709 Average score: 0.99 size: 0.01\n",
            "Step: 710 Average score: 1.0 size: 0.01\n",
            "Step: 711 Average score: 0.97 size: 0.01\n",
            "Step: 712 Average score: 0.99 size: 0.01\n",
            "Step: 713 Average score: 0.99 size: 0.01\n",
            "Step: 714 Average score: 0.98 size: 0.01\n",
            "Step: 715 Average score: 1.0 size: 0.01\n",
            "Step: 716 Average score: 0.99 size: 0.01\n",
            "Step: 717 Average score: 1.0 size: 0.01\n",
            "Step: 718 Average score: 0.96 size: 0.01\n",
            "Step: 719 Average score: 1.0 size: 0.01\n",
            "Step: 720 Average score: 0.99 size: 0.01\n",
            "Step: 721 Average score: 0.98 size: 0.01\n",
            "Step: 722 Average score: 0.99 size: 0.01\n",
            "Step: 723 Average score: 0.97 size: 0.01\n",
            "Step: 724 Average score: 0.99 size: 0.01\n",
            "Step: 725 Average score: 0.99 size: 0.01\n",
            "Step: 726 Average score: 1.0 size: 0.01\n",
            "Step: 727 Average score: 0.99 size: 0.01\n",
            "Step: 728 Average score: 0.98 size: 0.01\n",
            "Step: 729 Average score: 0.99 size: 0.01\n",
            "Step: 730 Average score: 1.0 size: 0.01\n",
            "Step: 731 Average score: 0.99 size: 0.01\n",
            "Step: 732 Average score: 0.97 size: 0.01\n",
            "Step: 733 Average score: 0.97 size: 0.01\n",
            "Step: 734 Average score: 0.99 size: 0.01\n",
            "Step: 735 Average score: 0.98 size: 0.01\n",
            "Step: 736 Average score: 1.0 size: 0.01\n",
            "Step: 737 Average score: 0.99 size: 0.01\n",
            "Step: 738 Average score: 0.98 size: 0.01\n",
            "Step: 739 Average score: 0.98 size: 0.01\n",
            "Step: 740 Average score: 0.98 size: 0.01\n",
            "Step: 741 Average score: 1.0 size: 0.01\n",
            "Step: 742 Average score: 0.98 size: 0.01\n",
            "Step: 743 Average score: 1.0 size: 0.01\n",
            "Step: 744 Average score: 0.99 size: 0.01\n",
            "Step: 745 Average score: 0.99 size: 0.01\n",
            "Step: 746 Average score: 1.0 size: 0.01\n",
            "Step: 747 Average score: 0.98 size: 0.01\n",
            "Step: 748 Average score: 0.97 size: 0.01\n",
            "Step: 749 Average score: 1.0 size: 0.01\n",
            "Step: 750 Average score: 0.98 size: 0.01\n",
            "Step: 751 Average score: 1.0 size: 0.01\n",
            "Step: 752 Average score: 0.98 size: 0.01\n",
            "Step: 753 Average score: 1.0 size: 0.01\n",
            "Step: 754 Average score: 0.95 size: 0.01\n",
            "Step: 755 Average score: 0.99 size: 0.01\n",
            "Step: 756 Average score: 0.99 size: 0.01\n",
            "Step: 757 Average score: 0.99 size: 0.01\n",
            "Step: 758 Average score: 1.0 size: 0.01\n",
            "Step: 759 Average score: 0.96 size: 0.01\n",
            "Step: 760 Average score: 0.97 size: 0.01\n",
            "Step: 761 Average score: 1.0 size: 0.01\n",
            "Step: 762 Average score: 0.99 size: 0.01\n",
            "Step: 763 Average score: 1.0 size: 0.01\n",
            "Step: 764 Average score: 0.99 size: 0.01\n",
            "Step: 765 Average score: 0.99 size: 0.01\n",
            "Step: 766 Average score: 0.99 size: 0.01\n",
            "Step: 767 Average score: 0.98 size: 0.01\n",
            "Step: 768 Average score: 0.99 size: 0.01\n",
            "Step: 769 Average score: 0.98 size: 0.01\n",
            "Step: 770 Average score: 1.0 size: 0.01\n",
            "Step: 771 Average score: 1.0 size: 0.01\n",
            "Step: 772 Average score: 1.0 size: 0.01\n",
            "Step: 773 Average score: 0.97 size: 0.01\n",
            "Step: 774 Average score: 0.99 size: 0.01\n",
            "Step: 775 Average score: 1.0 size: 0.01\n",
            "Step: 776 Average score: 0.98 size: 0.01\n",
            "Step: 777 Average score: 0.99 size: 0.01\n",
            "Step: 778 Average score: 0.99 size: 0.01\n",
            "Step: 779 Average score: 1.0 size: 0.01\n",
            "Step: 780 Average score: 0.97 size: 0.01\n",
            "Step: 781 Average score: 1.0 size: 0.01\n",
            "Step: 782 Average score: 0.98 size: 0.01\n",
            "Step: 783 Average score: 0.99 size: 0.01\n",
            "Step: 784 Average score: 0.97 size: 0.01\n",
            "Step: 785 Average score: 0.99 size: 0.01\n",
            "Step: 786 Average score: 0.97 size: 0.01\n",
            "Step: 787 Average score: 0.97 size: 0.01\n",
            "Step: 788 Average score: 1.0 size: 0.01\n",
            "Step: 789 Average score: 1.0 size: 0.01\n",
            "Step: 790 Average score: 0.99 size: 0.01\n",
            "Step: 791 Average score: 1.0 size: 0.01\n",
            "Step: 792 Average score: 0.97 size: 0.01\n",
            "Step: 793 Average score: 0.97 size: 0.01\n",
            "Step: 794 Average score: 1.0 size: 0.01\n",
            "Step: 795 Average score: 0.99 size: 0.01\n",
            "Step: 796 Average score: 0.99 size: 0.01\n",
            "Step: 797 Average score: 0.99 size: 0.01\n",
            "Step: 798 Average score: 1.0 size: 0.01\n",
            "Step: 799 Average score: 0.98 size: 0.01\n",
            "Step: 800 Average score: 1.0 size: 0.01\n",
            "Step: 801 Average score: 0.97 size: 0.01\n",
            "Step: 802 Average score: 0.98 size: 0.01\n",
            "Step: 803 Average score: 0.99 size: 0.01\n",
            "Step: 804 Average score: 1.0 size: 0.01\n",
            "Step: 805 Average score: 1.0 size: 0.01\n",
            "Step: 806 Average score: 0.98 size: 0.01\n",
            "Step: 807 Average score: 0.97 size: 0.01\n",
            "Step: 808 Average score: 0.98 size: 0.01\n",
            "Step: 809 Average score: 0.99 size: 0.01\n",
            "Step: 810 Average score: 1.0 size: 0.01\n",
            "Step: 811 Average score: 0.98 size: 0.01\n",
            "Step: 812 Average score: 0.98 size: 0.01\n",
            "Step: 813 Average score: 0.97 size: 0.01\n",
            "Step: 814 Average score: 0.99 size: 0.01\n",
            "Step: 815 Average score: 1.0 size: 0.01\n",
            "Step: 816 Average score: 1.0 size: 0.01\n",
            "Step: 817 Average score: 0.98 size: 0.01\n",
            "Step: 818 Average score: 0.99 size: 0.01\n",
            "Step: 819 Average score: 0.99 size: 0.01\n",
            "Step: 820 Average score: 0.99 size: 0.01\n",
            "Step: 821 Average score: 0.99 size: 0.01\n",
            "Step: 822 Average score: 1.0 size: 0.01\n",
            "Step: 823 Average score: 1.0 size: 0.01\n",
            "Step: 824 Average score: 0.99 size: 0.01\n",
            "Step: 825 Average score: 0.99 size: 0.01\n",
            "Step: 826 Average score: 0.95 size: 0.01\n",
            "Step: 827 Average score: 1.0 size: 0.01\n",
            "Step: 828 Average score: 0.98 size: 0.01\n",
            "Step: 829 Average score: 1.0 size: 0.01\n",
            "Step: 830 Average score: 1.0 size: 0.01\n",
            "Step: 831 Average score: 1.0 size: 0.01\n",
            "Step: 832 Average score: 1.0 size: 0.01\n",
            "Step: 833 Average score: 0.98 size: 0.01\n",
            "Step: 834 Average score: 1.0 size: 0.01\n",
            "Step: 835 Average score: 0.98 size: 0.01\n",
            "Step: 836 Average score: 0.98 size: 0.01\n",
            "Step: 837 Average score: 0.99 size: 0.01\n",
            "Step: 838 Average score: 0.99 size: 0.01\n",
            "Step: 839 Average score: 0.98 size: 0.01\n",
            "Step: 840 Average score: 0.99 size: 0.01\n",
            "Step: 841 Average score: 0.98 size: 0.01\n",
            "Step: 842 Average score: 1.0 size: 0.01\n",
            "Step: 843 Average score: 1.0 size: 0.01\n",
            "Step: 844 Average score: 1.0 size: 0.01\n",
            "Step: 845 Average score: 0.98 size: 0.01\n",
            "Step: 846 Average score: 0.99 size: 0.01\n",
            "Step: 847 Average score: 1.0 size: 0.01\n",
            "Step: 848 Average score: 0.99 size: 0.01\n",
            "Step: 849 Average score: 1.0 size: 0.01\n",
            "Step: 850 Average score: 0.94 size: 0.01\n",
            "Step: 851 Average score: 0.94 size: 0.01\n",
            "Step: 852 Average score: 0.98 size: 0.01\n",
            "Step: 853 Average score: 1.0 size: 0.01\n",
            "Step: 854 Average score: 0.98 size: 0.01\n",
            "Step: 855 Average score: 0.99 size: 0.01\n",
            "Step: 856 Average score: 1.0 size: 0.01\n",
            "Step: 857 Average score: 1.0 size: 0.01\n",
            "Step: 858 Average score: 0.98 size: 0.01\n",
            "Step: 859 Average score: 1.0 size: 0.01\n",
            "Step: 860 Average score: 0.99 size: 0.01\n",
            "Step: 861 Average score: 0.99 size: 0.01\n",
            "Step: 862 Average score: 1.0 size: 0.01\n",
            "Step: 863 Average score: 0.98 size: 0.01\n",
            "Step: 864 Average score: 0.98 size: 0.01\n",
            "Step: 865 Average score: 0.99 size: 0.01\n",
            "Step: 866 Average score: 0.99 size: 0.01\n",
            "Step: 867 Average score: 0.97 size: 0.01\n",
            "Step: 868 Average score: 1.0 size: 0.01\n",
            "Step: 869 Average score: 0.99 size: 0.01\n",
            "Step: 870 Average score: 1.0 size: 0.01\n",
            "Step: 871 Average score: 1.0 size: 0.01\n",
            "Step: 872 Average score: 1.0 size: 0.01\n",
            "Step: 873 Average score: 0.99 size: 0.01\n",
            "Step: 874 Average score: 1.0 size: 0.01\n",
            "Step: 875 Average score: 1.0 size: 0.01\n",
            "Step: 876 Average score: 0.98 size: 0.01\n",
            "Step: 877 Average score: 0.98 size: 0.01\n",
            "Step: 878 Average score: 0.99 size: 0.01\n",
            "Step: 879 Average score: 1.0 size: 0.01\n",
            "Step: 880 Average score: 0.96 size: 0.01\n",
            "Step: 881 Average score: 0.99 size: 0.01\n",
            "Step: 882 Average score: 0.99 size: 0.01\n",
            "Step: 883 Average score: 0.98 size: 0.01\n",
            "Step: 884 Average score: 0.96 size: 0.01\n",
            "Step: 885 Average score: 0.98 size: 0.01\n",
            "Step: 886 Average score: 1.0 size: 0.01\n",
            "Step: 887 Average score: 0.99 size: 0.01\n",
            "Step: 888 Average score: 0.99 size: 0.01\n",
            "Step: 889 Average score: 0.99 size: 0.01\n",
            "Step: 890 Average score: 1.0 size: 0.01\n",
            "Step: 891 Average score: 0.99 size: 0.01\n",
            "Step: 892 Average score: 0.97 size: 0.01\n",
            "Step: 893 Average score: 0.99 size: 0.01\n",
            "Step: 894 Average score: 0.99 size: 0.01\n",
            "Step: 895 Average score: 0.98 size: 0.01\n",
            "Step: 896 Average score: 1.0 size: 0.01\n",
            "Step: 897 Average score: 0.97 size: 0.01\n",
            "Step: 898 Average score: 1.0 size: 0.01\n",
            "Step: 899 Average score: 1.0 size: 0.01\n",
            "Step: 900 Average score: 0.98 size: 0.01\n",
            "Step: 901 Average score: 0.98 size: 0.01\n",
            "Step: 902 Average score: 0.99 size: 0.01\n",
            "Step: 903 Average score: 1.0 size: 0.01\n",
            "Step: 904 Average score: 0.97 size: 0.01\n",
            "Step: 905 Average score: 1.0 size: 0.01\n",
            "Step: 906 Average score: 0.98 size: 0.01\n",
            "Step: 907 Average score: 0.99 size: 0.01\n",
            "Step: 908 Average score: 0.98 size: 0.01\n",
            "Step: 909 Average score: 0.95 size: 0.01\n",
            "Step: 910 Average score: 1.0 size: 0.01\n",
            "Step: 911 Average score: 1.0 size: 0.01\n",
            "Step: 912 Average score: 1.0 size: 0.01\n",
            "Step: 913 Average score: 0.97 size: 0.01\n",
            "Step: 914 Average score: 0.99 size: 0.01\n",
            "Step: 915 Average score: 1.0 size: 0.01\n",
            "Step: 916 Average score: 0.97 size: 0.01\n",
            "Step: 917 Average score: 0.99 size: 0.01\n",
            "Step: 918 Average score: 0.99 size: 0.01\n",
            "Step: 919 Average score: 0.99 size: 0.01\n",
            "Step: 920 Average score: 1.0 size: 0.01\n",
            "Step: 921 Average score: 0.99 size: 0.01\n",
            "Step: 922 Average score: 0.98 size: 0.01\n",
            "Step: 923 Average score: 1.0 size: 0.01\n",
            "Step: 924 Average score: 0.99 size: 0.01\n",
            "Step: 925 Average score: 1.0 size: 0.01\n",
            "Step: 926 Average score: 0.99 size: 0.01\n",
            "Step: 927 Average score: 0.99 size: 0.01\n",
            "Step: 928 Average score: 0.98 size: 0.01\n",
            "Step: 929 Average score: 0.98 size: 0.01\n",
            "Step: 930 Average score: 0.99 size: 0.01\n",
            "Step: 931 Average score: 0.98 size: 0.01\n",
            "Step: 932 Average score: 0.97 size: 0.01\n",
            "Step: 933 Average score: 0.98 size: 0.01\n",
            "Step: 934 Average score: 0.97 size: 0.01\n",
            "Step: 935 Average score: 1.0 size: 0.01\n",
            "Step: 936 Average score: 1.0 size: 0.01\n",
            "Step: 937 Average score: 0.99 size: 0.01\n",
            "Step: 938 Average score: 1.0 size: 0.01\n",
            "Step: 939 Average score: 0.98 size: 0.01\n",
            "Step: 940 Average score: 0.99 size: 0.01\n",
            "Step: 941 Average score: 0.99 size: 0.01\n",
            "Step: 942 Average score: 0.99 size: 0.01\n",
            "Step: 943 Average score: 0.99 size: 0.01\n",
            "Step: 944 Average score: 0.98 size: 0.01\n",
            "Step: 945 Average score: 0.99 size: 0.01\n",
            "Step: 946 Average score: 0.98 size: 0.01\n",
            "Step: 947 Average score: 1.0 size: 0.01\n",
            "Step: 948 Average score: 1.0 size: 0.01\n",
            "Step: 949 Average score: 1.0 size: 0.01\n",
            "Step: 950 Average score: 1.0 size: 0.01\n",
            "Step: 951 Average score: 0.99 size: 0.01\n",
            "Step: 952 Average score: 0.99 size: 0.01\n",
            "Step: 953 Average score: 0.98 size: 0.01\n",
            "Step: 954 Average score: 0.99 size: 0.01\n",
            "Step: 955 Average score: 0.99 size: 0.01\n",
            "Step: 956 Average score: 1.0 size: 0.01\n",
            "Step: 957 Average score: 0.98 size: 0.01\n",
            "Step: 958 Average score: 0.99 size: 0.01\n",
            "Step: 959 Average score: 0.99 size: 0.01\n",
            "Step: 960 Average score: 0.96 size: 0.01\n",
            "Step: 961 Average score: 0.99 size: 0.01\n",
            "Step: 962 Average score: 0.99 size: 0.01\n",
            "Step: 963 Average score: 1.0 size: 0.01\n",
            "Step: 964 Average score: 1.0 size: 0.01\n",
            "Step: 965 Average score: 0.99 size: 0.01\n",
            "Step: 966 Average score: 1.0 size: 0.01\n",
            "Step: 967 Average score: 0.99 size: 0.01\n",
            "Step: 968 Average score: 0.97 size: 0.01\n",
            "Step: 969 Average score: 0.98 size: 0.01\n",
            "Step: 970 Average score: 0.97 size: 0.01\n",
            "Step: 971 Average score: 0.99 size: 0.01\n",
            "Step: 972 Average score: 0.99 size: 0.01\n",
            "Step: 973 Average score: 0.99 size: 0.01\n",
            "Step: 974 Average score: 0.99 size: 0.01\n",
            "Step: 975 Average score: 0.99 size: 0.01\n",
            "Step: 976 Average score: 1.0 size: 0.01\n",
            "Step: 977 Average score: 1.0 size: 0.01\n",
            "Step: 978 Average score: 1.0 size: 0.01\n",
            "Step: 979 Average score: 0.99 size: 0.01\n",
            "Step: 980 Average score: 0.98 size: 0.01\n",
            "Step: 981 Average score: 0.99 size: 0.01\n",
            "Step: 982 Average score: 0.99 size: 0.01\n",
            "Step: 983 Average score: 0.99 size: 0.01\n",
            "Step: 984 Average score: 0.99 size: 0.01\n",
            "Step: 985 Average score: 1.0 size: 0.01\n",
            "Step: 986 Average score: 0.98 size: 0.01\n",
            "Step: 987 Average score: 0.96 size: 0.01\n",
            "Step: 988 Average score: 0.99 size: 0.01\n",
            "Step: 989 Average score: 0.99 size: 0.01\n",
            "Step: 990 Average score: 1.0 size: 0.01\n",
            "Step: 991 Average score: 0.99 size: 0.01\n",
            "Step: 992 Average score: 1.0 size: 0.01\n",
            "Step: 993 Average score: 0.99 size: 0.01\n",
            "Step: 994 Average score: 0.99 size: 0.01\n",
            "Step: 995 Average score: 0.99 size: 0.01\n",
            "Step: 996 Average score: 0.97 size: 0.01\n",
            "Step: 997 Average score: 0.98 size: 0.01\n",
            "Step: 998 Average score: 1.0 size: 0.01\n",
            "Step: 999 Average score: 0.99 size: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XYQDuEd5XFj",
        "colab_type": "text"
      },
      "source": [
        "##Deep Q network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9SEdp0DjdA6",
        "colab_type": "text"
      },
      "source": [
        "**Objective**:\n",
        "Q-learning doesn't handle high-space dimension.\n",
        "We want to approximate the Q-function with a neural network\n",
        "But we can't just apply this because Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation and bootstrapping \n",
        "\n",
        "DQN aims to greatly improve and stabilize the training procedure of Q-learning with two mechanisme:\n",
        "\n",
        "**Two mechanisme**:\n",
        "- Replay Memory\n",
        "- Target Network\n",
        "\n",
        "**Replay Memory**:\n",
        "Each step of one episode $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in the replay-memory $D_t=(e_1,...,e_t)$ \n",
        "\n",
        "As we deal with neural network the data must be i.i.d, i.e Independent and identically distributed random variables. However with RL data is correleted and not identically distributed. \n",
        "Therefore we sample randomly episodes from the replay-memory\n",
        "\n",
        "\n",
        "**Target Network**:\n",
        "Q is optimized towards target values that are only periodically updated. The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). \n",
        "This modification makes the training more stable as it overcomes the short-term oscillations.\n",
        "\n",
        "\n",
        "**Loss function** : $L(\\theta) = E_{(s,a,r,s')~U(D)}[ ( r + \\gamma max_{a'}( Q (s, a; \\theta^-) ) - Q (s, a; \\theta) ]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQxCMi2tTjer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "HIDDEN_SIZE = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_KIG_rp5RJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions, learning_rate):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), \n",
        "                                    lr=learning_rate)\n",
        "        \n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "    def normalization(self, x):\n",
        "      x = torch.tensor(x).float().to(self.device)\n",
        "      #return F.normalize(x, p=2, dim=0)\n",
        "      return x\n",
        "    def forward(self, x):\n",
        "        x = self.normalization(x)\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ27pCOd-AiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "  def __init__(self, env, gamma, epsilon, learning_rate, batch_size,\n",
        "               target_update = 5, min_replay_size = 20,\n",
        "               epsilon_decay=0.999, epsilon_min =0.01, memory_size = 1_000_000 ):\n",
        "    self.env = env\n",
        "    self.n_actions = env.action_space.n\n",
        "    self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    self.replay = deque(maxlen = memory_size)\n",
        "\n",
        "    self.Q = Net(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q = Net(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_size = memory_size\n",
        "    self.min_replay_size = min_replay_size\n",
        "\n",
        "    self.target_update_counter = 0\n",
        "    self.target_update = target_update\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "        #Choose a action according to greedy epsilon\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "          action = np.random.choice(self.n_actions)\n",
        "        else:\n",
        "          action = torch.argmax(self.Q(obs)).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "  def train_nn(self):\n",
        "    if len(self.replay) < self.min_replay_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = [random.choice(self.replay) for _ in range(self.batch_size)]\n",
        "\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    actions = torch.tensor([transition[1] for transition in mini_batch]).to(self.Q.device)\n",
        "    rewards = torch.tensor([transition[2] for transition in mini_batch]).to(self.Q.device) \n",
        "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
        "    dones = torch.ByteTensor([not(transition[-1]) for transition in mini_batch]).to(self.Q.device)\n",
        "\n",
        "    next_state_values = self.target_Q(new_current_states).max(1)[0]\n",
        "    values = rewards + self.gamma*next_state_values*dones\n",
        "\n",
        "    target_values = self.Q(current_states).gather(dim=1,index=actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "   \n",
        "    #fit/backpropagation\n",
        "    self.Q.optimizer.zero_grad()\n",
        "    loss_t = self.Q.loss_function(values, target_values)\n",
        "    loss_t.backward()\n",
        "    self.Q.optimizer.step()\n",
        "    #Udate target every C step\n",
        "    self.update_target()\n",
        "    #Update epsilon\n",
        "    self.update_epsilon()\n",
        "   \n",
        "  def update_epsilon(self):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_update_counter +=1\n",
        "    if self.target_update_counter > self.target_update:\n",
        "       self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "       self.target_update_counter = 0\n",
        "\n",
        "       \n",
        "  def reward_shipping(self, reward):\n",
        "    if reward > 1:\n",
        "      return 1\n",
        "    if reward < -1:\n",
        "      return -1\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def train(self, episodes_batch): \n",
        "    for episode in range(episodes_batch):\n",
        "\n",
        "      #initialized sequence\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not(done):\n",
        "        action = self.choose_action(obs)\n",
        "        #perform action\n",
        "        next_obs, reward, done, _ = self.env.step(action)           \n",
        "        #store transition in replay\n",
        "        self.replay.append((obs, action, reward, next_obs, done))\n",
        "        #train neural network\n",
        "        self.train_nn()\n",
        "\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "     \n",
        "      #print(\"step:\", episode,\"reward:\", total_reward,\"eps:\", self.epsilon)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAgEkY_v9tmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "f5580afc-64fe-4a09-cb33-0334ef16f890"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = DQNAgent(env, gamma=0.99, epsilon=0.9, learning_rate=0.01, batch_size = 20)\n",
        "agent.train(episodes_batch=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1fc5edd9a815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-22fb70cc4501>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes_batch)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m#train neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-22fb70cc4501>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#Udate target every C step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdHX_YLjVkB",
        "colab_type": "text"
      },
      "source": [
        "## Double DQN Agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIcPC1Bt6Vm5",
        "colab_type": "text"
      },
      "source": [
        "The idea of Double Q-learning is to reduce overestimations\n",
        "by decomposing the max operation in the target into action\n",
        "selection and action evaluation\n",
        "We just replace the target by this expression\n",
        "\n",
        "$Y_t = R_{t+1} + \\gamma Q (S_{t+1}, argmax_a( Q (S_{t+1}, a, \\theta_t),  \\theta^-_t) ) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ84Tl5BJ6qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions, learning_rate):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), \n",
        "                                    lr=learning_rate)\n",
        "        \n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "        \n",
        "    def normalization(self, x):\n",
        "      x = torch.tensor(x).float().to(self.device)\n",
        "      #return F.normalize(x, p=2, dim=0)\n",
        "      return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalization(x)\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8f_0VhqDimJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DDQNAgent:\n",
        "  def __init__(self, env, gamma, epsilon, learning_rate, batch_size,\n",
        "               target_update = 5, min_replay_size = 20,\n",
        "               epsilon_decay=0.995, epsilon_min =0.01, memory_size = 1_000_000 ):\n",
        "    self.env = env\n",
        "    self.n_actions = env.action_space.n\n",
        "    self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    self.replay = deque(maxlen = memory_size)\n",
        "\n",
        "    self.Q = Net(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q = Net(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_size = memory_size\n",
        "    self.min_replay_size = min_replay_size\n",
        "\n",
        "    self.target_update_counter = 0\n",
        "    self.target_update = target_update\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "        #Choose a action according to greedy epsilon\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "          action = np.random.choice(self.n_actions)\n",
        "        else:\n",
        "          action = torch.argmax(self.Q(obs)).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "  def train_nn(self):\n",
        "    if len(self.replay) < self.min_replay_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = [random.choice(self.replay) for _ in range(self.batch_size)]\n",
        "\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    actions = torch.tensor([transition[1] for transition in mini_batch]).to(self.Q.device)\n",
        "    rewards = torch.tensor([transition[2] for transition in mini_batch]).to(self.Q.device)  \n",
        "    new_current_states = torch.tensor([transition[3] for transition in mini_batch]).to(self.Q.device)\n",
        "    dones = torch.ByteTensor([not(transition[-1]) for transition in mini_batch]).to(self.Q.device)\n",
        "\n",
        "#\n",
        "    fq =  torch.argmax(self.Q(new_current_states), dim=1)\n",
        "#\n",
        "\n",
        "    next_state_values = self.target_Q(new_current_states).gather(dim=1, index=fq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    values = rewards + self.gamma*next_state_values*dones\n",
        "\n",
        "    target_values = self.Q(current_states).gather(dim=1,index=actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "   \n",
        "    #fit/backpropagation\n",
        "    self.Q.optimizer.zero_grad()\n",
        "    loss_t = self.Q.loss_function(values, target_values)\n",
        "    loss_t.backward()\n",
        "    self.Q.optimizer.step()\n",
        "    #Udate target every C step\n",
        "    self.update_target()\n",
        "    #Update epsilon\n",
        "    self.update_epsilon()\n",
        "   \n",
        "  def update_epsilon(self):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_update_counter +=1\n",
        "    if self.target_update_counter > self.target_update:\n",
        "       self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "       self.target_update_counter = 0\n",
        "\n",
        "  def reward_shipping(self, reward):\n",
        "    if reward > 1:\n",
        "      return 1\n",
        "    if reward < -1:\n",
        "      return -1\n",
        "    return reward\n",
        "  def train(self, episodes_batch): \n",
        "    for episode in range(episodes_batch):\n",
        "\n",
        "      #initialized sequence\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not(done):\n",
        "        action = self.choose_action(obs)\n",
        "        #perform action\n",
        "        next_obs, reward, done, _ = self.env.step(action)           \n",
        "        #store transition in replay\n",
        "        self.replay.append((obs, action, self.reward_shipping(reward), next_obs, done))\n",
        "        #train neural network\n",
        "        self.train_nn()\n",
        "\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "     \n",
        "      #print(\"step:\", episode,\"reward:\", total_reward,\"eps:\", self.epsilon)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfBJNtqnB15s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "4d5066c6-c9f1-4c34-cc67-c7c8b429850f"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = DDQNAgent(env, gamma=0.99, epsilon=0.9, learning_rate=0.01, batch_size = 20)\n",
        "agent.train(episodes_batch=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9a61099b43a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4cd1f83aab8e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, gamma, epsilon, learning_rate, batch_size, target_update, min_replay_size, epsilon_decay, epsilon_min, memory_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9ddcc9f94c38>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obs_size, hidden_size, n_actions, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9vugUBI6kjg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRlroQqT6L82",
        "colab_type": "text"
      },
      "source": [
        "##Dueling Q network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldJOfC0H6dCx",
        "colab_type": "text"
      },
      "source": [
        "The key motivation behind this architecture is that for some games, it is unnecessary to know the value of each action at every timestep.\n",
        "\n",
        "\n",
        "The dueling Q network is just a changing in the network architecture, the RL algorithme doesn't change.\n",
        "The new network architecture is this one:\n",
        "\n",
        "![Texte alternatif](https://miro.medium.com/max/1400/1*GKZ-cS0mCdXMOO_bfBlN0Q.png)\n",
        "\n",
        "\n",
        "The network is seperate in two MLP. One will predict approch the V function, the other the Advantage function:\n",
        "Advantage function:\n",
        "\n",
        "$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$\n",
        "\n",
        "The Aggregation layer is :\n",
        "$Q(s,a;\\theta,\\alpha,\\beta) = V(s;\\theta,\\beta) + (A(s,a:\\theta,\\alpha) - \\frac{1}{|A|}\\sum_{a'}A(s,a;\\theta,\\alpha))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jLOMmEH6Tos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingNet(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions, learning_rate):\n",
        "        super(DuelingNet, self).__init__()\n",
        "\n",
        "        self.net_value = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        self.net_advantage = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), \n",
        "                                    lr=learning_rate)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "    def normalization(self, x):\n",
        "      x = torch.tensor(x).float().to(self.device)\n",
        "      #return F.normalize(x, p=2, dim=0)\n",
        "      return x\n",
        "    def forward(self, x):\n",
        "        x = self.normalization(x)\n",
        "        values = self.net_value(x)\n",
        "        advantages = self.net_advantage(x)\n",
        "        qvals = values + (advantages - advantages.mean())\n",
        "        \n",
        "        return qvals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FrfzLb1FqYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class D3QNAgent:\n",
        "  def __init__(self, env, gamma, epsilon, learning_rate, batch_size,\n",
        "               target_update = 5, min_replay_size = 20,\n",
        "               epsilon_decay=0.995, epsilon_min =0.01, memory_size = 1_000_000 ):\n",
        "    self.env = env\n",
        "    self.n_actions = env.action_space.n\n",
        "    self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    self.replay = deque(maxlen = memory_size)\n",
        "\n",
        "    self.Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_size = memory_size\n",
        "    self.min_replay_size = min_replay_size\n",
        "\n",
        "    self.target_update_counter = 0\n",
        "    self.target_update = target_update\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "        #Choose a action according to greedy epsilon\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "          action = np.random.choice(self.n_actions)\n",
        "        else:\n",
        "          action = torch.argmax(self.Q(obs)).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "  def train_nn(self):\n",
        "    if len(self.replay) < self.min_replay_size:\n",
        "      return\n",
        "\n",
        "    mini_batch = [random.choice(self.replay) for _ in range(self.batch_size)]\n",
        "\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    actions = torch.tensor([transition[1] for transition in mini_batch]).to(self.Q.device)\n",
        "    rewards = torch.tensor([transition[2] for transition in mini_batch]).to(self.Q.device) \n",
        "    new_current_states = torch.tensor([transition[3] for transition in mini_batch]).to(self.Q.device)\n",
        "    dones = torch.ByteTensor([not(transition[-1]) for transition in mini_batch]).to(self.Q.device)\n",
        "\n",
        "\n",
        "    fq =  torch.argmax(self.Q(new_current_states), dim=1)\n",
        "\n",
        "    next_state_values = self.target_Q(new_current_states).gather(dim=1, index=fq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    values = rewards + self.gamma*next_state_values*dones\n",
        "\n",
        "    target_values = self.Q(current_states).gather(dim=1,index=actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "   \n",
        "    #fit/backpropagation\n",
        "    self.Q.optimizer.zero_grad()\n",
        "    loss_t = self.Q.loss_function(values, target_values)\n",
        "    loss_t.backward()\n",
        "    self.Q.optimizer.step()\n",
        "    #Udate target every C step\n",
        "    self.update_target()\n",
        "    #Update epsilon\n",
        "    self.update_epsilon()\n",
        "   \n",
        "  def update_epsilon(self):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_update_counter +=1\n",
        "    if self.target_update_counter > self.target_update:\n",
        "       self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "       self.target_update_counter = 0\n",
        "       \n",
        "  def reward_shipping(self, reward):\n",
        "    if reward > 1:\n",
        "      return 1\n",
        "    if reward < -1:\n",
        "      return -1\n",
        "    return reward\n",
        "\n",
        "  def train(self, episodes_batch): \n",
        "    for episode in range(episodes_batch):\n",
        "\n",
        "      #initialized sequence\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not(done):\n",
        "        action = self.choose_action(obs)\n",
        "        #perform action\n",
        "        next_obs, reward, done, _ = self.env.step(action)           \n",
        "        #store transition in replay\n",
        "        self.replay.append((obs, action, self.reward_shipping(reward), next_obs, done))\n",
        "        #train neural network\n",
        "        self.train_nn()\n",
        "\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "     \n",
        "      #print(\"step:\", episode,\"reward:\", total_reward,\"eps:\", self.epsilon)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSn87Jed7QlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "77a24fb6-2e87-4539-c1f2-519c34bf659b"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = D3QNAgent(env, gamma=0.99, epsilon=0.9, learning_rate=0.01, batch_size = 20)\n",
        "agent.train(episodes_batch=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3e515063ddea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD3QNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-dc7f9e2973ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes_batch)\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;31m#perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-dc7f9e2973ed>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6078a54ab3ae>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_advantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mqvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKgNWHIn3GVd",
        "colab_type": "text"
      },
      "source": [
        "## D3QN with priotise experience replay (under development)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iZgQE9B9Zg3",
        "colab_type": "text"
      },
      "source": [
        "###Version array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAQcyWqozfow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import normalize\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "HIDDEN_SIZE = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGcQSzFb7WGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingNet(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions, learning_rate):\n",
        "        super(DuelingNet, self).__init__()\n",
        "\n",
        "        self.net_value = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        self.net_advantage = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), \n",
        "                                    lr=learning_rate)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "    def normalization(self, x):\n",
        "      x = torch.tensor(x).float().to(self.device)\n",
        "      #return F.normalize(x, p=2, dim=0)\n",
        "      return x\n",
        "    def forward(self, x):\n",
        "        x = self.normalization(x)\n",
        "        values = self.net_value(x)\n",
        "        advantages = self.net_advantage(x)\n",
        "        qvals = values + (advantages - advantages.mean())\n",
        "        \n",
        "        return qvals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgfYrGNb3NWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory:\n",
        "  def __init__(self, limite, alpha, beta, epsilon):\n",
        "    self.transitions = deque(maxlen = limite)\n",
        "    self.priority = deque(maxlen = limite)\n",
        "\n",
        "\n",
        "    self.probability = deque(maxlen=limite)\n",
        "    self.td_error = deque(maxlen=limite)\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.index_sample = None\n",
        "\n",
        "    \n",
        "  def add_experience(self, experience):\n",
        "    priority = max(self.priority) if self.priority  else 1\n",
        "    \n",
        "    self.priority.append(priority)\n",
        "    self.transitions.append(experience)\n",
        "  \n",
        "  def update_error(self, error):\n",
        "    t = (abs(error.detach().cpu() )+ self.epsilon)\n",
        "    np.array(self.priority)[self.index_sample] = t**(self.alpha)\n",
        "\n",
        "  def get_batch(self, size):\n",
        "    t = time.time()\n",
        "    self.probability = np.array(self.priority)/sum(self.priority)\n",
        "    N = len(self.transitions)\n",
        "    index_sample = np.random.choice(N, size, p=self.probability, replace=True)\n",
        "    #s = sum(np.array(self.priority)[index_sample])\n",
        "    \n",
        "    mini_batch = np.array(self.transitions)[index_sample]\n",
        "    p = np.array(self.probability)[index_sample]\n",
        "    weight = (N*p)**(-self.beta)\n",
        "    isw =  weight/max(weight)\n",
        "    self.index_sample = index_sample\n",
        "    #print(time.time()- t)\n",
        "    return mini_batch, isw\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqqE8yl33Qv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class D3QNAgentV:\n",
        "  def __init__(self, env, gamma, epsilon, learning_rate, batch_size,\n",
        "               target_update = 5, min_replay_size = 20,\n",
        "               epsilon_decay=0.995, epsilon_min =0.01, memory_size = 1_000_000 ):\n",
        "    self.env = env\n",
        "    self.n_actions = env.action_space.n\n",
        "    self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    self.replay = Memory(memory_size, 0.6, 0.4, 0.00001)\n",
        "\n",
        "    self.Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_size = memory_size\n",
        "    self.min_replay_size = min_replay_size\n",
        "\n",
        "    self.target_update_counter = 0\n",
        "    self.target_update = target_update\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "        #Choose a action according to greedy epsilon\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "          action = np.random.choice(self.n_actions)\n",
        "        else:\n",
        "          action = torch.argmax(self.Q(obs)).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "  def train_nn(self):\n",
        "    if len(self.replay.transitions) < self.min_replay_size:\n",
        "      return\n",
        "\n",
        "    mini_batch, weight = self.replay.get_batch(self.batch_size)\n",
        "\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    actions = torch.tensor([transition[1] for transition in mini_batch]).to(self.Q.device)\n",
        "    rewards = torch.tensor([transition[2] for transition in mini_batch]).to(self.Q.device) \n",
        "    new_current_states = torch.tensor([transition[3] for transition in mini_batch]).to(self.Q.device)\n",
        "    dones = torch.ByteTensor([not(transition[4]) for transition in mini_batch]).to(self.Q.device)\n",
        "    weight = torch.tensor(weight).to(self.Q.device)\n",
        "\n",
        "#\n",
        "    fq =  torch.argmax(self.Q(new_current_states), dim=1)\n",
        "#\n",
        "\n",
        "    next_state_values = self.target_Q(new_current_states).gather(dim=1, index=fq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    values = (rewards + self.gamma*next_state_values*dones)\n",
        "\n",
        "    target_values =(self.Q(current_states).gather(dim=1,index=actions.unsqueeze(-1)).squeeze(-1))\n",
        "\n",
        "\n",
        "    td_error = target_values - values\n",
        "    self.replay.update_error(td_error)\n",
        "   \n",
        "    #fit/backpropagation\n",
        "    self.Q.optimizer.zero_grad()\n",
        "    loss_t = self.Q.loss_function(values*weight, target_values*weight) \n",
        "    loss_t.backward()\n",
        "    self.Q.optimizer.step()\n",
        "\n",
        "    #self.replay.update_memory()\n",
        "    self.update_epsilon()\n",
        "    self.update_target()\n",
        "    self.update_hyp()\n",
        "\n",
        "  def update_hyp(self):\n",
        "    self.replay.beta = 1-self.epsilon \n",
        "  def update_epsilon(self):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_update_counter +=1\n",
        "    if self.target_update_counter > self.target_update:\n",
        "       self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "       self.target_update_counter = 0\n",
        "       \n",
        "  def reward_shipping(self, reward):\n",
        "    if reward > 1:\n",
        "      return 1\n",
        "    if reward < -1:\n",
        "      return -1\n",
        "    return reward\n",
        "\n",
        "  def train(self, episodes_batch): \n",
        "    for episode in range(episodes_batch):\n",
        "\n",
        "      #initialized sequence\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not(done):\n",
        "        action = self.choose_action(obs)\n",
        "        #perform action\n",
        "        next_obs, reward, done, _ = self.env.step(action)           \n",
        "        #store transition in replay\n",
        "        self.replay.add_experience([obs, action, reward, next_obs, done])\n",
        "        #train neural network\n",
        "        self.train_nn()\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "     \n",
        "      print(\"step:\", episode,\"reward:\", total_reward,\"eps:\", self.epsilon)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl-g8DD2N0T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "6d05a2ab-86ee-4999-d16f-a41cace89df3"
      },
      "source": [
        "import time\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = D3QNAgentV(env, gamma=0.99, epsilon=0.9, learning_rate=0.01, batch_size = 20)\n",
        "agent.train(episodes_batch=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 0 reward: 19.0 eps: 0.9\n",
            "step: 1 reward: 19.0 eps: 0.8182406354242773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step: 2 reward: 15.0 eps: 0.7589746224457501\n",
            "step: 3 reward: 15.0 eps: 0.7040013079012841\n",
            "step: 4 reward: 27.0 eps: 0.6148888899491377\n",
            "step: 5 reward: 37.0 eps: 0.5107997002143378\n",
            "step: 6 reward: 24.0 eps: 0.4529023473580582\n",
            "step: 7 reward: 14.0 eps: 0.42220920936111966\n",
            "step: 8 reward: 20.0 eps: 0.38193487565652895\n",
            "step: 9 reward: 36.0 eps: 0.3188748180463633\n",
            "step: 10 reward: 18.0 eps: 0.2913638567716998\n",
            "step: 11 reward: 87.0 eps: 0.1883838646522939\n",
            "step: 12 reward: 79.0 eps: 0.1267847682173169\n",
            "step: 13 reward: 165.0 eps: 0.055446661768577966\n",
            "step: 14 reward: 170.0 eps: 0.02364825669251974\n",
            "step: 15 reward: 92.0 eps: 0.014911542115010218\n",
            "step: 16 reward: 73.0 eps: 0.010342069545760784\n",
            "step: 17 reward: 93.0 eps: 0.01\n",
            "step: 18 reward: 158.0 eps: 0.01\n",
            "step: 19 reward: 128.0 eps: 0.01\n",
            "step: 20 reward: 105.0 eps: 0.01\n",
            "step: 21 reward: 183.0 eps: 0.01\n",
            "step: 22 reward: 200.0 eps: 0.01\n",
            "step: 23 reward: 166.0 eps: 0.01\n",
            "step: 24 reward: 200.0 eps: 0.01\n",
            "step: 25 reward: 200.0 eps: 0.01\n",
            "step: 26 reward: 190.0 eps: 0.01\n",
            "step: 27 reward: 200.0 eps: 0.01\n",
            "step: 28 reward: 186.0 eps: 0.01\n",
            "step: 29 reward: 200.0 eps: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6tMkqs_9c7f",
        "colab_type": "text"
      },
      "source": [
        "###Version Sum tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lKRzf3d9hAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import normalize\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import deque\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "HIDDEN_SIZE = 256\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1RXqFF9ijG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingNet(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions, learning_rate):\n",
        "        super(DuelingNet, self).__init__()\n",
        "\n",
        "        self.net_value = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        self.net_advantage = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(params=self.parameters(), \n",
        "                                    lr=learning_rate)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(self.device)\n",
        "\n",
        "    def normalization(self, x):\n",
        "      x = torch.tensor(x).float().to(self.device)\n",
        "      #return F.normalize(x, p=2, dim=0)\n",
        "      return x\n",
        "    def forward(self, x):\n",
        "        x = self.normalization(x)\n",
        "        values = self.net_value(x)\n",
        "        advantages = self.net_advantage(x)\n",
        "        qvals = values + (advantages - advantages.mean())\n",
        "        \n",
        "        return qvals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaYA6FWP9mos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class sum_tree:\n",
        "  def __init__(self, capacity):\n",
        "   \n",
        "    self.data = deque(maxlen=capacity)\n",
        "    self.priorities = np.zeros(shape=capacity)\n",
        "    self.steep_n = math.log(len(self.priorities))/math.log(2)\n",
        "    self.size = capacity\n",
        "    self.leaf = np.zeros(shape=(int(2**self.steep_n)-1)+capacity)\n",
        "    \n",
        "    self.steep_n = math.log(len(self.priorities))/math.log(2)\n",
        "    self.id_up = 2**(len(self.priorities)//4)\n",
        "    self.idx = 0\n",
        "    \n",
        "    \n",
        "    self.leaf_n = len(self.leaf) - capacity\n",
        "\n",
        "  def create_tree(self):\n",
        "    leaf = []\n",
        "    tp =  self.priorities\n",
        "    for i in range(1, self.size):\n",
        "      tmp=[]\n",
        "      for i in range(self.size//2**i):\n",
        "        tmp.append(sum(tp[2*i:2*i+2]))\n",
        "      \n",
        "      tp = tmp\n",
        "      tmp.extend(leaf)\n",
        "      leaf = tmp\n",
        "    \n",
        "    leaf.extend(self.priorities)\n",
        "    return leaf\n",
        "\n",
        "  def update_tree(self, idx, element):\n",
        "    idx_leaf = self.leaf_n + idx\n",
        "    self.leaf[idx_leaf] = element\n",
        "\n",
        "    idx_parent = int((idx_leaf - 1)/2)\n",
        "\n",
        "    for i in range(int(self.steep_n)):\n",
        "      idx_g = (2*idx_parent) +1\n",
        "      idx_d = (2*idx_parent) +2\n",
        "      self.leaf[idx_parent] = self.leaf[idx_g] + self.leaf[idx_d]\n",
        "\n",
        "      idx_parent = int((idx_parent - 1)/2)\n",
        "\n",
        "  def push(self, element):\n",
        "    self.data.append(element[0])\n",
        "    self.priorities[self.idx] = element[1]\n",
        "    self.update_tree(self.idx, element[1])\n",
        "\n",
        "    self.idx += 1\n",
        "    self.idx %= self.size\n",
        "    \n",
        "  \n",
        "  \n",
        "  def sample_idx(self):\n",
        "      value = np.random.uniform()*self.leaf[0]\n",
        "      idx = 0\n",
        "      for i in range(int(self.steep_n)):\n",
        "        if value < self.leaf[(2*idx)+1]:\n",
        "          idx = (2*idx)+1\n",
        "        else:   \n",
        "          value = value - self.leaf[(2*idx)+1]\n",
        "          idx = (2*idx)+2\n",
        "      if  -(self.leaf_n - idx)==1048575:\n",
        "        return 0\n",
        "      else:\n",
        "        return -(self.leaf_n - idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIpYwa8c9opV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MemoryTree:\n",
        "  def __init__(self, limite, alpha, beta, epsilon):\n",
        "    self.transitions = sum_tree(limite)\n",
        "\n",
        "    self.sampling_weight = []\n",
        "\n",
        "    self.probability = deque(maxlen=limite)\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.limit = limite\n",
        "    self.max_prio = 1\n",
        "    \n",
        "  def add_experience(self, experience):\n",
        "    prio = self.max_prio\n",
        "    experience.append(len(self.transitions.data)-1)\n",
        "    self.transitions.push((experience, prio))\n",
        "    \n",
        "  \n",
        "  def update_error(self, error, index):\n",
        "    for el in zip(error, index):\n",
        "      new_p = (abs(el[0].item()) + self.epsilon)**(self.alpha)\n",
        "      self.max_prio = max(self.max_prio, new_p)\n",
        "      self.transitions.update_tree(el[1].item(), new_p)\n",
        "    \n",
        "\n",
        "  def update_probability(self):\n",
        "    s = sum(self.transitions.priorities)\n",
        "    self.probability =  self.transitions.priorities/s\n",
        "\n",
        "  def get_batch(self, size):\n",
        "    #self.update_probability()\n",
        "    t = time.time()\n",
        "    N = len(self.transitions.data)\n",
        "\n",
        "    index_sample = [self.transitions.sample_idx() for i in range(size)]\n",
        "    prio_batch = np.array(self.transitions.priorities)[index_sample]\n",
        "\n",
        "    mini_batch = np.array(self.transitions.data)[index_sample]\n",
        "    weight = (N*np.array(prio_batch))**(-self.beta)\n",
        "\n",
        "    mw = max(weight)\n",
        "    isw = weight/mw \n",
        "\n",
        "    #print(time.time()-t)\n",
        "    return mini_batch, isw\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGXFH0PF9qc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class D3QNPERAgent:\n",
        "  def __init__(self, env, gamma, epsilon, learning_rate, batch_size,\n",
        "               target_update = 5, min_replay_size = 20,\n",
        "               epsilon_decay=0.995, epsilon_min =0.01, memory_size = 1_048_576 ):\n",
        "    self.env = env\n",
        "    self.n_actions = env.action_space.n\n",
        "    self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    self.replay = MemoryTree(memory_size, 0.6, 0.4, 0.00001)\n",
        "\n",
        "    self.Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q = DuelingNet(self.obs_size, HIDDEN_SIZE, self.n_actions, learning_rate)\n",
        "    self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.memory_size = memory_size\n",
        "    self.min_replay_size = min_replay_size\n",
        "\n",
        "    self.target_update_counter = 0\n",
        "    self.target_update = target_update\n",
        "\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "        #Choose a action according to greedy epsilon\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "          action = np.random.choice(self.n_actions)\n",
        "        else:\n",
        "          action = torch.argmax(self.Q(obs)).item()\n",
        "        return action\n",
        "\n",
        "\n",
        "  def train_nn(self):\n",
        "    if len(self.replay.transitions.data) < self.min_replay_size:\n",
        "      return\n",
        "\n",
        "    mini_batch, weight = self.replay.get_batch(self.batch_size)\n",
        "\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    actions = torch.tensor([transition[1] for transition in mini_batch]).to(self.Q.device)\n",
        "    rewards = torch.tensor([transition[2] for transition in mini_batch]).to(self.Q.device) \n",
        "    new_current_states = torch.tensor([transition[3] for transition in mini_batch]).to(self.Q.device)\n",
        "    dones = torch.ByteTensor([not(transition[4]) for transition in mini_batch]).to(self.Q.device)\n",
        "    index_transitions = torch.tensor([transition[5] for transition in mini_batch]).to(self.Q.device)\n",
        "    weight = torch.tensor(weight).to(self.Q.device)\n",
        "\n",
        "#\n",
        "    fq =  torch.argmax(self.Q(new_current_states), dim=1)\n",
        "#\n",
        "\n",
        "    next_state_values = self.target_Q(new_current_states).gather(dim=1, index=fq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    values = (rewards + self.gamma*next_state_values*dones)\n",
        "\n",
        "    target_values =(self.Q(current_states).gather(dim=1,index=actions.unsqueeze(-1)).squeeze(-1))\n",
        "\n",
        "\n",
        "    td_error = target_values - values\n",
        "    self.replay.update_error(td_error, index_transitions)\n",
        "   \n",
        "    #fit/backpropagation\n",
        "    self.Q.optimizer.zero_grad()\n",
        "    loss_t = self.Q.loss_function(values*weight, target_values*weight) \n",
        "    loss_t.backward()\n",
        "    self.Q.optimizer.step()\n",
        "\n",
        "    #self.replay.update_memory()\n",
        "    self.update_epsilon()\n",
        "    self.update_target()\n",
        "    self.update_hyp()\n",
        "\n",
        "  def update_hyp(self):\n",
        "    self.replay.beta = 1-self.epsilon \n",
        "  def update_epsilon(self):\n",
        "    self.epsilon *= self.epsilon_decay\n",
        "    self.epsilon = max(self.epsilon, self.epsilon_min)\n",
        "\n",
        "  def update_target(self):\n",
        "    self.target_update_counter +=1\n",
        "    if self.target_update_counter > self.target_update:\n",
        "       self.target_Q.load_state_dict(self.Q.state_dict())\n",
        "       self.target_update_counter = 0\n",
        "\n",
        "       \n",
        "  def reward_shipping(self, reward):\n",
        "    if reward > 1:\n",
        "      return 1\n",
        "    if reward < -1:\n",
        "      return -1\n",
        "    return reward\n",
        "\n",
        "  def train(self, episodes_batch): \n",
        "    for episode in range(episodes_batch):\n",
        "\n",
        "      #initialized sequence\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not(done):\n",
        "        action = self.choose_action(obs)\n",
        "        #perform action\n",
        "        next_obs, reward, done, _ = self.env.step(action)           \n",
        "        #store transition in replay\n",
        "        self.replay.add_experience([obs, action, reward, next_obs, done])\n",
        "        #train neural network\n",
        "        self.train_nn()\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        if total_reward > 200:\n",
        "          break;\n",
        "\n",
        "     \n",
        "      print(\"step:\", episode,\"reward:\", total_reward,\"eps:\", self.epsilon)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdi_s5p-hQjo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b5afdfa-8d41-4a44-b940-43fb894ae795"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = D3QNPERAgent(env, gamma=0.99, epsilon=0.9, learning_rate=0.01, batch_size = 20)\n",
        "agent.train(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step: 0 reward: 30.0 eps: 0.8517191218320987\n",
            "step: 1 reward: 12.0 eps: 0.801998150202188\n",
            "step: 2 reward: 16.0 eps: 0.7401890541913636\n",
            "step: 3 reward: 13.0 eps: 0.693494000380694\n",
            "step: 4 reward: 25.0 eps: 0.611814445499392\n",
            "step: 5 reward: 18.0 eps: 0.55903008447479\n",
            "step: 6 reward: 42.0 eps: 0.4529023473580582\n",
            "step: 7 reward: 36.0 eps: 0.37812507527185507\n",
            "step: 8 reward: 42.0 eps: 0.30634081946852443\n",
            "step: 9 reward: 9.0 eps: 0.29282799675547716\n",
            "step: 10 reward: 12.0 eps: 0.2757335202480061\n",
            "step: 11 reward: 10.0 eps: 0.2622529444168676\n",
            "step: 12 reward: 137.0 eps: 0.1319721934755129\n",
            "step: 13 reward: 200.0 eps: 0.04842822864619853\n",
            "step: 14 reward: 144.0 eps: 0.02353001540905714\n",
            "step: 15 reward: 111.0 eps: 0.013489137274294582\n",
            "step: 16 reward: 155.0 eps: 0.01\n",
            "step: 17 reward: 150.0 eps: 0.01\n",
            "step: 18 reward: 200.0 eps: 0.01\n",
            "step: 19 reward: 102.0 eps: 0.01\n",
            "step: 20 reward: 200.0 eps: 0.01\n",
            "step: 21 reward: 200.0 eps: 0.01\n",
            "step: 22 reward: 200.0 eps: 0.01\n",
            "step: 23 reward: 88.0 eps: 0.01\n",
            "step: 24 reward: 9.0 eps: 0.01\n",
            "step: 25 reward: 10.0 eps: 0.01\n",
            "step: 26 reward: 63.0 eps: 0.01\n",
            "step: 27 reward: 75.0 eps: 0.01\n",
            "step: 28 reward: 56.0 eps: 0.01\n",
            "step: 29 reward: 10.0 eps: 0.01\n",
            "step: 30 reward: 200.0 eps: 0.01\n",
            "step: 31 reward: 83.0 eps: 0.01\n",
            "step: 32 reward: 200.0 eps: 0.01\n",
            "step: 33 reward: 47.0 eps: 0.01\n",
            "step: 34 reward: 112.0 eps: 0.01\n",
            "step: 35 reward: 18.0 eps: 0.01\n",
            "step: 36 reward: 10.0 eps: 0.01\n",
            "step: 37 reward: 9.0 eps: 0.01\n",
            "step: 38 reward: 124.0 eps: 0.01\n",
            "step: 39 reward: 10.0 eps: 0.01\n",
            "step: 40 reward: 10.0 eps: 0.01\n",
            "step: 41 reward: 9.0 eps: 0.01\n",
            "step: 42 reward: 10.0 eps: 0.01\n",
            "step: 43 reward: 9.0 eps: 0.01\n",
            "step: 44 reward: 9.0 eps: 0.01\n",
            "step: 45 reward: 9.0 eps: 0.01\n",
            "step: 46 reward: 11.0 eps: 0.01\n",
            "step: 47 reward: 9.0 eps: 0.01\n",
            "step: 48 reward: 9.0 eps: 0.01\n",
            "step: 49 reward: 10.0 eps: 0.01\n",
            "step: 50 reward: 9.0 eps: 0.01\n",
            "step: 51 reward: 12.0 eps: 0.01\n",
            "step: 52 reward: 10.0 eps: 0.01\n",
            "step: 53 reward: 21.0 eps: 0.01\n",
            "step: 54 reward: 72.0 eps: 0.01\n",
            "step: 55 reward: 11.0 eps: 0.01\n",
            "step: 56 reward: 10.0 eps: 0.01\n",
            "step: 57 reward: 11.0 eps: 0.01\n",
            "step: 58 reward: 9.0 eps: 0.01\n",
            "step: 59 reward: 9.0 eps: 0.01\n",
            "step: 60 reward: 8.0 eps: 0.01\n",
            "step: 61 reward: 9.0 eps: 0.01\n",
            "step: 62 reward: 10.0 eps: 0.01\n",
            "step: 63 reward: 52.0 eps: 0.01\n",
            "step: 64 reward: 200.0 eps: 0.01\n",
            "step: 65 reward: 200.0 eps: 0.01\n",
            "step: 66 reward: 200.0 eps: 0.01\n",
            "step: 67 reward: 34.0 eps: 0.01\n",
            "step: 68 reward: 163.0 eps: 0.01\n",
            "step: 69 reward: 200.0 eps: 0.01\n",
            "step: 70 reward: 197.0 eps: 0.01\n",
            "step: 71 reward: 76.0 eps: 0.01\n",
            "step: 72 reward: 100.0 eps: 0.01\n",
            "step: 73 reward: 200.0 eps: 0.01\n",
            "step: 74 reward: 186.0 eps: 0.01\n",
            "step: 75 reward: 143.0 eps: 0.01\n",
            "step: 76 reward: 200.0 eps: 0.01\n",
            "step: 77 reward: 72.0 eps: 0.01\n",
            "step: 78 reward: 192.0 eps: 0.01\n",
            "step: 79 reward: 200.0 eps: 0.01\n",
            "step: 80 reward: 200.0 eps: 0.01\n",
            "step: 81 reward: 200.0 eps: 0.01\n",
            "step: 82 reward: 125.0 eps: 0.01\n",
            "step: 83 reward: 30.0 eps: 0.01\n",
            "step: 84 reward: 27.0 eps: 0.01\n",
            "step: 85 reward: 25.0 eps: 0.01\n",
            "step: 86 reward: 34.0 eps: 0.01\n",
            "step: 87 reward: 52.0 eps: 0.01\n",
            "step: 88 reward: 41.0 eps: 0.01\n",
            "step: 89 reward: 37.0 eps: 0.01\n",
            "step: 90 reward: 35.0 eps: 0.01\n",
            "step: 91 reward: 26.0 eps: 0.01\n",
            "step: 92 reward: 28.0 eps: 0.01\n",
            "step: 93 reward: 63.0 eps: 0.01\n",
            "step: 94 reward: 136.0 eps: 0.01\n",
            "step: 95 reward: 157.0 eps: 0.01\n",
            "step: 96 reward: 200.0 eps: 0.01\n",
            "step: 97 reward: 105.0 eps: 0.01\n",
            "step: 98 reward: 67.0 eps: 0.01\n",
            "step: 99 reward: 48.0 eps: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rMtbvjObtau",
        "colab_type": "text"
      },
      "source": [
        "#Policy-based Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u43pKBIVb6T",
        "colab_type": "text"
      },
      "source": [
        "No value function\n",
        "Learn policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDXcuUiO-a8y",
        "colab_type": "text"
      },
      "source": [
        "# Actor-Critic Algorithms"
      ]
    }
  ]
}